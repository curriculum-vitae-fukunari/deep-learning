{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 日本語の文章をBag of Wordsにするまで"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Wordsは係り受けの情報は考えず、文中の単語とその出現頻度にのみ着目した文のベクトル表現。  \n",
    "分類タスクには使えるが、時系列は無視してしまっているので、生成系は工夫が必要だろう。  \n",
    "\n",
    "また、ストップワードの除去や次元の削減は行う必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from MeCab import Tagger\n",
    "\n",
    "from gensim import corpora, matutils\n",
    "\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mecabでテキストを形態素解析し、.mecabで保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/fukunaritakeshi/PycharmProjects/deep-learning/data/neko.txt\", \"r\") as file:\n",
    "    lines: List[str] = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u3000どこで生れたかとんと見当がつかぬ。\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger: Tagger = MeCab.Tagger()\n",
    "sentence: str = tagger.parse(lines[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u3000\\t記号,空白,*,*,*,*,\\u3000,\\u3000,\\u3000\\nどこ\\t名詞,代名詞,一般,*,*,*,どこ,ドコ,ドコ\\nで\\t助詞,格助詞,一般,*,*,*,で,デ,デ\\n生れ\\t動詞,自立,*,*,一段,連用形,生れる,ウマレ,ウマレ\\nた\\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\\nか\\t助詞,副助詞／並立助詞／終助詞,*,*,*,*,か,カ,カ\\nとんと\\t副詞,一般,*,*,*,*,とんと,トント,トント\\n見当\\t名詞,サ変接続,*,*,*,*,見当,ケントウ,ケントー\\nが\\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\\nつか\\t動詞,自立,*,*,五段・カ行イ音便,未然形,つく,ツカ,ツカ\\nぬ\\t助動詞,*,*,*,特殊・ヌ,基本形,ぬ,ヌ,ヌ\\n。\\t記号,句点,*,*,*,*,。,。,。\\nEOS\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = sentence.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\u3000\\t記号,空白,*,*,*,*,\\u3000,\\u3000,\\u3000',\n 'どこ\\t名詞,代名詞,一般,*,*,*,どこ,ドコ,ドコ',\n 'で\\t助詞,格助詞,一般,*,*,*,で,デ,デ',\n '生れ\\t動詞,自立,*,*,一段,連用形,生れる,ウマレ,ウマレ',\n 'た\\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ',\n 'か\\t助詞,副助詞／並立助詞／終助詞,*,*,*,*,か,カ,カ',\n 'とんと\\t副詞,一般,*,*,*,*,とんと,トント,トント',\n '見当\\t名詞,サ変接続,*,*,*,*,見当,ケントウ,ケントー',\n 'が\\t助詞,格助詞,一般,*,*,*,が,ガ,ガ',\n 'つか\\t動詞,自立,*,*,五段・カ行イ音便,未然形,つく,ツカ,ツカ',\n 'ぬ\\t助動詞,*,*,*,特殊・ヌ,基本形,ぬ,ヌ,ヌ',\n '。\\t記号,句点,*,*,*,*,。,。,。',\n 'EOS',\n '']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_list: List[str] = []\n",
    "for word in word_list:\n",
    "    surface: str = word.split(\"\\t\")[0]\n",
    "    surface_list.append(surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\u3000',\n 'どこ',\n 'で',\n '生れ',\n 'た',\n 'か',\n 'とんと',\n '見当',\n 'が',\n 'つか',\n 'ぬ',\n '。',\n 'EOS',\n '']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surface_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_list.remove(\"EOS\")\n",
    "surface_list.remove(\"\")\n",
    "surface_list.remove(\"\\u3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['どこ', 'で', '生れ', 'た', 'か', 'とんと', '見当', 'が', 'つか', 'ぬ', '。']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surface_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでの一連の流れをtokenize関数として定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(*, text_file_path: str) -> List[List[str]]:\n",
    "    \n",
    "    with open(text_file_path, \"r\") as file:\n",
    "        lines: List[str] = file.readlines()\n",
    "    \n",
    "    tagger: Tagger = MeCab.Tagger()    \n",
    "    surface_lists: List[List[str]] = []\n",
    "    for line in lines:\n",
    "        sentence: str = tagger.parse(line)\n",
    "        word_list = sentence.split(\"\\n\")\n",
    "        \n",
    "        surface_list: List[str] = []\n",
    "        for word in word_list:\n",
    "            surface: str = word.split(\"\\t\")[0]\n",
    "            if not(surface == \"EOS\" or surface == \"\" or surface == \"\\u3000\"):\n",
    "                surface_list.append(surface)\n",
    "        \n",
    "        # 空行や一単語のものは除外する\n",
    "        if len(surface_list) > 1:\n",
    "            surface_lists.append(surface_list)   \n",
    "        \n",
    "    return surface_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_lists: List[List[str]] = tokenize(text_file_path=\"/Users/fukunaritakeshi/PycharmProjects/deep-learning/data/neko.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['この',\n '書生',\n 'という',\n 'の',\n 'は',\n '時々',\n '我々',\n 'を',\n '捕え',\n 'て',\n '煮',\n 'て',\n '食う',\n 'という',\n '話',\n 'で',\n 'ある',\n '。']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surface_lists[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリGensimを用いて特徴語の辞書をつくる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(surface_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辞書の保存や読み込みもできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path: str = \"/Users/fukunaritakeshi/PycharmProjects/deep-learning/data/neko_dictionary.txt\"\n",
    "dictionary.save_as_text(dictionary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load_from_text(dictionary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴ベクトルを作成。\n",
    "\n",
    "辞書中の単語idと頻度のタプルとなっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['人間', 'の', '日記', 'の', '本', '色', 'は', 'こう', '云う', '辺', 'に', '存する', 'の', 'かも', '知れ', 'ない', '。']\n[(0, 1), (3, 1), (35, 1), (53, 3), (72, 1), (111, 1), (238, 1), (263, 1), (268, 1), (354, 1), (424, 1), (436, 1), (1377, 1), (2207, 1), (2291, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(surface_lists[503])\n",
    "feature_vector_tuple = dictionary.doc2bow(surface_lists[503])\n",
    "print(feature_vector_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトルの表現に変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n 0.0,\n 0.0,\n 1.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector = list(matutils.corpus2dense([feature_vector_tuple], num_terms=len(dictionary)).T[0])\n",
    "feature_vector[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの流れをfeature_vector関数にし、全テキストに適用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(*, token_lists: List[List[str]]) -> List[List[float]]:\n",
    "    \n",
    "    dictionary = corpora.Dictionary(token_lists)\n",
    "    \n",
    "    feature_vector_list: List[List[float]] = []\n",
    "    for token_list in token_lists:\n",
    "        feature_vector_tuple = dictionary.doc2bow(token_list)\n",
    "        feature_vector: List[float] = list(matutils.corpus2dense([feature_vector_tuple], num_terms=len(dictionary)).T[0])\n",
    "        feature_vector_list.append(feature_vector)\n",
    "        \n",
    "    return feature_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_list: List[List[float]] = vectorize(token_lists=surface_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector_list[500][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
