{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手書き文字のRNN(LSTM)による分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像の認識はCNNで行うのが主流であるが、自身の学習も兼ねて、  \n",
    "手書き文字画像をあえてRNNで分類する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# tensorflow関係\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import tflearn.datasets.mnist as mnist\n",
    "from tflearn.datasets.mnist import DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの取得・前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 解凍がうまくいかない場合がある。mnist.read_data_setsをたたくとうまくいくのか\n",
    "trainX, trainY, testX, testY = mnist.load_data(one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像データは以下のようになっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "        0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "        0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "        0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "        0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "        0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "        0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "        0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "        0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "        0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "        0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "        0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "        0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "        0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "        0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "        0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "        0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "        0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "        0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "        0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "        0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "        0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "        0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "        0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "        0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解データは、数値の大小の概念がないため、one-hotエンコーディングにより以下のように表現される。  \n",
    "以下は7であることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像ピクセルデータを1次元から系列データに変換する。\n",
    "\n",
    "reshape関数を使ってサイズが784の一次元配列を、1次元配列28個の系列データに変換する。  \n",
    "引数の-1はなんだ？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (-1, 28, 28))\n",
    "testX = np.reshape(testX, (-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のように2次元配列に変換されたことがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.38039219,  0.37647063,  0.3019608 ,  0.46274513,\n",
       "         0.2392157 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.35294119,  0.5411765 ,\n",
       "         0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "         0.92156869,  0.98431379,  0.98431379,  0.97254908,  0.99607849,\n",
       "         0.96078438,  0.92156869,  0.74509805,  0.08235294,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.54901963,  0.98431379,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.74117649,  0.09019608,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "         0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "         0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "         0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "         0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.14901961,  0.32156864,  0.0509804 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.13333334,  0.83529419,  0.99607849,  0.99607849,\n",
       "         0.45098042,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "         0.91764712,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "         0.91764712,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.41568631,  0.6156863 ,  0.99607849,  0.99607849,\n",
       "         0.95294124,  0.20000002,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "         0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.26666668,\n",
       "         0.4666667 ,  0.86274517,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.55686277,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.14509805,  0.73333335,  0.99215692,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.87450987,  0.80784321,\n",
       "         0.80784321,  0.29411766,  0.26666668,  0.84313732,  0.99607849,\n",
       "         0.99607849,  0.45882356,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.44313729,  0.8588236 ,  0.99607849,  0.94901967,\n",
       "         0.89019614,  0.45098042,  0.34901962,  0.12156864,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.7843138 ,  0.99607849,\n",
       "         0.9450981 ,  0.16078432,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.66274512,  0.99607849,  0.6901961 ,  0.24313727,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.18823531,  0.90588242,  0.99607849,\n",
       "         0.91764712,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "         0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.54509807,  0.99607849,  0.9333334 ,\n",
       "         0.22352943,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.82352948,  0.98039222,  0.99607849,  0.65882355,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.94901967,  0.99607849,  0.93725497,  0.22352943,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.34901962,  0.98431379,  0.9450981 ,  0.33725491,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "         0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.01568628,\n",
       "         0.45882356,  0.27058825,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力層28×28ノード、中間層LSTM128ノード、出力層10ノード(0〜9 を分類するため)の  \n",
    "RNNを構築する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期化を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力層を作成する。\n",
    "\n",
    "shapeには入力する学習データの形状としてバッチサイズとノード数を設定する。  \n",
    "バッチサイズは指定せず(None)、ノード数は28×28(一枚の画像のピクセル数)とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tflearn.input_data(shape=[None, 28, 28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間層(LSTMブロック)を作成する。\n",
    "\n",
    "incoming: 結合の対象(作成する一つ前)の層  \n",
    "n_units: LSTMのノード数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tflearn.lstm(incoming=net, n_units=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力層(全結合層)を作成する。\n",
    "\n",
    "incoming: 結合の対象(作成する一つ前)の層  \n",
    "n_units: LSTMのノード数  \n",
    "activation: 活性化関数。ここではソフトマックス関数を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tflearn.fully_connected(incoming=net, n_units=10, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の条件を設定する。\n",
    "\n",
    "incoming: 学習の対象となる層  \n",
    "optimizer: 最適化手法。ここではsgd(確率的勾配降下法)を使用する。  \n",
    "learning_rate: 学習率？  \n",
    "loss: 誤差関数。ここではcategorial_clossentropy(交差エントロピー)を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tflearn.regression(incoming=net, \n",
    "                         optimizer=\"sgd\", \n",
    "                         learning_rate=0.5, \n",
    "                         loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN関数により作成したRNNネットワークと学習条件をセットする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tflearn.DNN(network=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit関数を用いて学習を実行し、モデルを作成する。\n",
    "\n",
    "X_inputs: 学習データ。ここでは学習用の画像ピクセルデータ。  \n",
    "Y_targets: 正解データ。\n",
    "n_epoch: 学習回数。\n",
    "batch_size: バッチサイズ。　\n",
    "validation_set: モデルの精度を検証するためのテストデータセットの割合。ここでは学習データセットのうち1割をテストデータとする。  \n",
    "show_metric: 学習のステップごとに精度を表示するかどうかの設定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 0U5NNT\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 49500\n",
      "Validation samples: 5500\n",
      "--\n",
      "Training Step: 1  | time: 1.036s\n",
      "| SGD | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 01000/49500\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m2.07265\u001b[0m\u001b[0m | time: 1.757s\n",
      "| SGD | epoch: 001 | loss: 2.07265 - acc: 0.0927 -- iter: 02000/49500\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m2.25965\u001b[0m\u001b[0m | time: 2.586s\n",
      "| SGD | epoch: 001 | loss: 2.25965 - acc: 0.1036 -- iter: 03000/49500\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m2.28900\u001b[0m\u001b[0m | time: 3.292s\n",
      "| SGD | epoch: 001 | loss: 2.28900 - acc: 0.1324 -- iter: 04000/49500\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m2.29505\u001b[0m\u001b[0m | time: 3.880s\n",
      "| SGD | epoch: 001 | loss: 2.29505 - acc: 0.1390 -- iter: 05000/49500\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m2.29657\u001b[0m\u001b[0m | time: 4.436s\n",
      "| SGD | epoch: 001 | loss: 2.29657 - acc: 0.1474 -- iter: 06000/49500\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m2.29498\u001b[0m\u001b[0m | time: 5.184s\n",
      "| SGD | epoch: 001 | loss: 2.29498 - acc: 0.1609 -- iter: 07000/49500\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m2.29337\u001b[0m\u001b[0m | time: 5.796s\n",
      "| SGD | epoch: 001 | loss: 2.29337 - acc: 0.1700 -- iter: 08000/49500\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m2.29235\u001b[0m\u001b[0m | time: 6.390s\n",
      "| SGD | epoch: 001 | loss: 2.29235 - acc: 0.1631 -- iter: 09000/49500\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m2.29020\u001b[0m\u001b[0m | time: 6.978s\n",
      "| SGD | epoch: 001 | loss: 2.29020 - acc: 0.1711 -- iter: 10000/49500\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m2.29004\u001b[0m\u001b[0m | time: 7.545s\n",
      "| SGD | epoch: 001 | loss: 2.29004 - acc: 0.1649 -- iter: 11000/49500\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m2.28739\u001b[0m\u001b[0m | time: 8.095s\n",
      "| SGD | epoch: 001 | loss: 2.28739 - acc: 0.1933 -- iter: 12000/49500\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m2.28602\u001b[0m\u001b[0m | time: 8.669s\n",
      "| SGD | epoch: 001 | loss: 2.28602 - acc: 0.2120 -- iter: 13000/49500\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m2.28302\u001b[0m\u001b[0m | time: 9.218s\n",
      "| SGD | epoch: 001 | loss: 2.28302 - acc: 0.2398 -- iter: 14000/49500\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m2.27949\u001b[0m\u001b[0m | time: 9.789s\n",
      "| SGD | epoch: 001 | loss: 2.27949 - acc: 0.2622 -- iter: 15000/49500\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m2.27696\u001b[0m\u001b[0m | time: 10.333s\n",
      "| SGD | epoch: 001 | loss: 2.27696 - acc: 0.2655 -- iter: 16000/49500\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m2.27421\u001b[0m\u001b[0m | time: 10.936s\n",
      "| SGD | epoch: 001 | loss: 2.27421 - acc: 0.2635 -- iter: 17000/49500\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m2.26999\u001b[0m\u001b[0m | time: 11.566s\n",
      "| SGD | epoch: 001 | loss: 2.26999 - acc: 0.2651 -- iter: 18000/49500\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m2.27306\u001b[0m\u001b[0m | time: 12.215s\n",
      "| SGD | epoch: 001 | loss: 2.27306 - acc: 0.2444 -- iter: 19000/49500\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m2.26504\u001b[0m\u001b[0m | time: 12.900s\n",
      "| SGD | epoch: 001 | loss: 2.26504 - acc: 0.2623 -- iter: 20000/49500\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m2.25775\u001b[0m\u001b[0m | time: 13.590s\n",
      "| SGD | epoch: 001 | loss: 2.25775 - acc: 0.2550 -- iter: 21000/49500\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m2.24514\u001b[0m\u001b[0m | time: 14.241s\n",
      "| SGD | epoch: 001 | loss: 2.24514 - acc: 0.2550 -- iter: 22000/49500\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m2.23269\u001b[0m\u001b[0m | time: 14.868s\n",
      "| SGD | epoch: 001 | loss: 2.23269 - acc: 0.2475 -- iter: 23000/49500\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m2.21666\u001b[0m\u001b[0m | time: 15.428s\n",
      "| SGD | epoch: 001 | loss: 2.21666 - acc: 0.2454 -- iter: 24000/49500\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m2.19693\u001b[0m\u001b[0m | time: 15.984s\n",
      "| SGD | epoch: 001 | loss: 2.19693 - acc: 0.2464 -- iter: 25000/49500\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m2.17180\u001b[0m\u001b[0m | time: 16.550s\n",
      "| SGD | epoch: 001 | loss: 2.17180 - acc: 0.2497 -- iter: 26000/49500\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m2.14608\u001b[0m\u001b[0m | time: 17.100s\n",
      "| SGD | epoch: 001 | loss: 2.14608 - acc: 0.2531 -- iter: 27000/49500\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m2.14261\u001b[0m\u001b[0m | time: 17.673s\n",
      "| SGD | epoch: 001 | loss: 2.14261 - acc: 0.2421 -- iter: 28000/49500\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m2.17618\u001b[0m\u001b[0m | time: 18.221s\n",
      "| SGD | epoch: 001 | loss: 2.17618 - acc: 0.2173 -- iter: 29000/49500\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m2.16641\u001b[0m\u001b[0m | time: 18.819s\n",
      "| SGD | epoch: 001 | loss: 2.16641 - acc: 0.2118 -- iter: 30000/49500\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m2.13713\u001b[0m\u001b[0m | time: 19.389s\n",
      "| SGD | epoch: 001 | loss: 2.13713 - acc: 0.2480 -- iter: 31000/49500\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m2.11420\u001b[0m\u001b[0m | time: 19.980s\n",
      "| SGD | epoch: 001 | loss: 2.11420 - acc: 0.2618 -- iter: 32000/49500\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m2.08984\u001b[0m\u001b[0m | time: 20.550s\n",
      "| SGD | epoch: 001 | loss: 2.08984 - acc: 0.2712 -- iter: 33000/49500\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m2.05676\u001b[0m\u001b[0m | time: 21.109s\n",
      "| SGD | epoch: 001 | loss: 2.05676 - acc: 0.2856 -- iter: 34000/49500\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m2.02660\u001b[0m\u001b[0m | time: 21.690s\n",
      "| SGD | epoch: 001 | loss: 2.02660 - acc: 0.2976 -- iter: 35000/49500\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m2.01823\u001b[0m\u001b[0m | time: 22.247s\n",
      "| SGD | epoch: 001 | loss: 2.01823 - acc: 0.2962 -- iter: 36000/49500\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m2.06004\u001b[0m\u001b[0m | time: 22.823s\n",
      "| SGD | epoch: 001 | loss: 2.06004 - acc: 0.2680 -- iter: 37000/49500\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m2.06734\u001b[0m\u001b[0m | time: 23.405s\n",
      "| SGD | epoch: 001 | loss: 2.06734 - acc: 0.2553 -- iter: 38000/49500\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m2.04043\u001b[0m\u001b[0m | time: 24.000s\n",
      "| SGD | epoch: 001 | loss: 2.04043 - acc: 0.2654 -- iter: 39000/49500\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m2.01914\u001b[0m\u001b[0m | time: 24.569s\n",
      "| SGD | epoch: 001 | loss: 2.01914 - acc: 0.2816 -- iter: 40000/49500\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.99401\u001b[0m\u001b[0m | time: 25.136s\n",
      "| SGD | epoch: 001 | loss: 1.99401 - acc: 0.2951 -- iter: 41000/49500\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.97907\u001b[0m\u001b[0m | time: 25.728s\n",
      "| SGD | epoch: 001 | loss: 1.97907 - acc: 0.3026 -- iter: 42000/49500\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.94845\u001b[0m\u001b[0m | time: 26.299s\n",
      "| SGD | epoch: 001 | loss: 1.94845 - acc: 0.3121 -- iter: 43000/49500\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.92720\u001b[0m\u001b[0m | time: 27.192s\n",
      "| SGD | epoch: 001 | loss: 1.92720 - acc: 0.3249 -- iter: 44000/49500\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.90514\u001b[0m\u001b[0m | time: 28.485s\n",
      "| SGD | epoch: 001 | loss: 1.90514 - acc: 0.3313 -- iter: 45000/49500\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.89509\u001b[0m\u001b[0m | time: 29.986s\n",
      "| SGD | epoch: 001 | loss: 1.89509 - acc: 0.3359 -- iter: 46000/49500\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.88624\u001b[0m\u001b[0m | time: 31.032s\n",
      "| SGD | epoch: 001 | loss: 1.88624 - acc: 0.3353 -- iter: 47000/49500\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.88323\u001b[0m\u001b[0m | time: 32.061s\n",
      "| SGD | epoch: 001 | loss: 1.88323 - acc: 0.3348 -- iter: 48000/49500\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.86592\u001b[0m\u001b[0m | time: 32.848s\n",
      "| SGD | epoch: 001 | loss: 1.86592 - acc: 0.3433 -- iter: 49000/49500\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.84214\u001b[0m\u001b[0m | time: 34.494s\n",
      "| SGD | epoch: 001 | loss: 1.84214 - acc: 0.3636 | val_loss: 1.85843 - val_acc: 0.3758 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.83406\u001b[0m\u001b[0m | time: 0.321s\n",
      "| SGD | epoch: 002 | loss: 1.83406 - acc: 0.3661 -- iter: 01000/49500\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.83534\u001b[0m\u001b[0m | time: 0.919s\n",
      "| SGD | epoch: 002 | loss: 1.83534 - acc: 0.3715 -- iter: 02000/49500\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.83065\u001b[0m\u001b[0m | time: 1.508s\n",
      "| SGD | epoch: 002 | loss: 1.83065 - acc: 0.3649 -- iter: 03000/49500\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.83045\u001b[0m\u001b[0m | time: 2.112s\n",
      "| SGD | epoch: 002 | loss: 1.83045 - acc: 0.3707 -- iter: 04000/49500\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.80761\u001b[0m\u001b[0m | time: 2.693s\n",
      "| SGD | epoch: 002 | loss: 1.80761 - acc: 0.3806 -- iter: 05000/49500\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.78501\u001b[0m\u001b[0m | time: 3.299s\n",
      "| SGD | epoch: 002 | loss: 1.78501 - acc: 0.3926 -- iter: 06000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.77190\u001b[0m\u001b[0m | time: 3.896s\n",
      "| SGD | epoch: 002 | loss: 1.77190 - acc: 0.3930 -- iter: 07000/49500\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.75108\u001b[0m\u001b[0m | time: 4.487s\n",
      "| SGD | epoch: 002 | loss: 1.75108 - acc: 0.4031 -- iter: 08000/49500\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.75667\u001b[0m\u001b[0m | time: 5.075s\n",
      "| SGD | epoch: 002 | loss: 1.75667 - acc: 0.3965 -- iter: 09000/49500\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.74589\u001b[0m\u001b[0m | time: 5.645s\n",
      "| SGD | epoch: 002 | loss: 1.74589 - acc: 0.4012 -- iter: 10000/49500\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.74782\u001b[0m\u001b[0m | time: 6.240s\n",
      "| SGD | epoch: 002 | loss: 1.74782 - acc: 0.3971 -- iter: 11000/49500\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.72598\u001b[0m\u001b[0m | time: 6.828s\n",
      "| SGD | epoch: 002 | loss: 1.72598 - acc: 0.4021 -- iter: 12000/49500\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.71752\u001b[0m\u001b[0m | time: 7.497s\n",
      "| SGD | epoch: 002 | loss: 1.71752 - acc: 0.3998 -- iter: 13000/49500\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.70788\u001b[0m\u001b[0m | time: 8.079s\n",
      "| SGD | epoch: 002 | loss: 1.70788 - acc: 0.4080 -- iter: 14000/49500\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.74871\u001b[0m\u001b[0m | time: 8.711s\n",
      "| SGD | epoch: 002 | loss: 1.74871 - acc: 0.3969 -- iter: 15000/49500\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.81505\u001b[0m\u001b[0m | time: 9.364s\n",
      "| SGD | epoch: 002 | loss: 1.81505 - acc: 0.3696 -- iter: 16000/49500\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.78796\u001b[0m\u001b[0m | time: 10.047s\n",
      "| SGD | epoch: 002 | loss: 1.78796 - acc: 0.3838 -- iter: 17000/49500\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.75299\u001b[0m\u001b[0m | time: 10.698s\n",
      "| SGD | epoch: 002 | loss: 1.75299 - acc: 0.3993 -- iter: 18000/49500\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.72558\u001b[0m\u001b[0m | time: 11.377s\n",
      "| SGD | epoch: 002 | loss: 1.72558 - acc: 0.4110 -- iter: 19000/49500\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.86897\u001b[0m\u001b[0m | time: 12.056s\n",
      "| SGD | epoch: 002 | loss: 1.86897 - acc: 0.3771 -- iter: 20000/49500\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m1.84447\u001b[0m\u001b[0m | time: 12.622s\n",
      "| SGD | epoch: 002 | loss: 1.84447 - acc: 0.3861 -- iter: 21000/49500\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.81434\u001b[0m\u001b[0m | time: 13.204s\n",
      "| SGD | epoch: 002 | loss: 1.81434 - acc: 0.3938 -- iter: 22000/49500\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.77848\u001b[0m\u001b[0m | time: 13.762s\n",
      "| SGD | epoch: 002 | loss: 1.77848 - acc: 0.4056 -- iter: 23000/49500\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.74863\u001b[0m\u001b[0m | time: 14.334s\n",
      "| SGD | epoch: 002 | loss: 1.74863 - acc: 0.4137 -- iter: 24000/49500\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.71648\u001b[0m\u001b[0m | time: 14.908s\n",
      "| SGD | epoch: 002 | loss: 1.71648 - acc: 0.4268 -- iter: 25000/49500\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.69851\u001b[0m\u001b[0m | time: 15.480s\n",
      "| SGD | epoch: 002 | loss: 1.69851 - acc: 0.4299 -- iter: 26000/49500\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m1.66262\u001b[0m\u001b[0m | time: 16.056s\n",
      "| SGD | epoch: 002 | loss: 1.66262 - acc: 0.4431 -- iter: 27000/49500\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m1.62631\u001b[0m\u001b[0m | time: 16.609s\n",
      "| SGD | epoch: 002 | loss: 1.62631 - acc: 0.4576 -- iter: 28000/49500\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m1.59353\u001b[0m\u001b[0m | time: 17.186s\n",
      "| SGD | epoch: 002 | loss: 1.59353 - acc: 0.4668 -- iter: 29000/49500\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m1.57609\u001b[0m\u001b[0m | time: 17.751s\n",
      "| SGD | epoch: 002 | loss: 1.57609 - acc: 0.4704 -- iter: 30000/49500\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m1.55829\u001b[0m\u001b[0m | time: 18.359s\n",
      "| SGD | epoch: 002 | loss: 1.55829 - acc: 0.4722 -- iter: 31000/49500\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m1.55147\u001b[0m\u001b[0m | time: 18.938s\n",
      "| SGD | epoch: 002 | loss: 1.55147 - acc: 0.4718 -- iter: 32000/49500\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m1.58203\u001b[0m\u001b[0m | time: 19.494s\n",
      "| SGD | epoch: 002 | loss: 1.58203 - acc: 0.4621 -- iter: 33000/49500\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m1.60843\u001b[0m\u001b[0m | time: 20.078s\n",
      "| SGD | epoch: 002 | loss: 1.60843 - acc: 0.4502 -- iter: 34000/49500\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m1.57745\u001b[0m\u001b[0m | time: 20.638s\n",
      "| SGD | epoch: 002 | loss: 1.57745 - acc: 0.4607 -- iter: 35000/49500\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m1.54733\u001b[0m\u001b[0m | time: 21.223s\n",
      "| SGD | epoch: 002 | loss: 1.54733 - acc: 0.4733 -- iter: 36000/49500\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m1.51969\u001b[0m\u001b[0m | time: 21.835s\n",
      "| SGD | epoch: 002 | loss: 1.51969 - acc: 0.4830 -- iter: 37000/49500\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m1.50485\u001b[0m\u001b[0m | time: 22.442s\n",
      "| SGD | epoch: 002 | loss: 1.50485 - acc: 0.4870 -- iter: 38000/49500\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m1.51694\u001b[0m\u001b[0m | time: 23.040s\n",
      "| SGD | epoch: 002 | loss: 1.51694 - acc: 0.4827 -- iter: 39000/49500\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m1.51410\u001b[0m\u001b[0m | time: 23.628s\n",
      "| SGD | epoch: 002 | loss: 1.51410 - acc: 0.4820 -- iter: 40000/49500\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m1.51709\u001b[0m\u001b[0m | time: 24.236s\n",
      "| SGD | epoch: 002 | loss: 1.51709 - acc: 0.4755 -- iter: 41000/49500\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m1.51351\u001b[0m\u001b[0m | time: 24.830s\n",
      "| SGD | epoch: 002 | loss: 1.51351 - acc: 0.4769 -- iter: 42000/49500\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m1.51932\u001b[0m\u001b[0m | time: 25.554s\n",
      "| SGD | epoch: 002 | loss: 1.51932 - acc: 0.4735 -- iter: 43000/49500\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m1.53223\u001b[0m\u001b[0m | time: 26.261s\n",
      "| SGD | epoch: 002 | loss: 1.53223 - acc: 0.4714 -- iter: 44000/49500\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m1.50244\u001b[0m\u001b[0m | time: 26.958s\n",
      "| SGD | epoch: 002 | loss: 1.50244 - acc: 0.4813 -- iter: 45000/49500\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m1.47367\u001b[0m\u001b[0m | time: 27.706s\n",
      "| SGD | epoch: 002 | loss: 1.47367 - acc: 0.4920 -- iter: 46000/49500\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m1.44093\u001b[0m\u001b[0m | time: 28.329s\n",
      "| SGD | epoch: 002 | loss: 1.44093 - acc: 0.5069 -- iter: 47000/49500\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m1.40267\u001b[0m\u001b[0m | time: 28.945s\n",
      "| SGD | epoch: 002 | loss: 1.40267 - acc: 0.5207 -- iter: 48000/49500\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m1.37762\u001b[0m\u001b[0m | time: 29.535s\n",
      "| SGD | epoch: 002 | loss: 1.37762 - acc: 0.5285 -- iter: 49000/49500\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.35840\u001b[0m\u001b[0m | time: 31.262s\n",
      "| SGD | epoch: 002 | loss: 1.35840 - acc: 0.5351 | val_loss: 1.22267 - val_acc: 0.5735 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m1.35945\u001b[0m\u001b[0m | time: 0.321s\n",
      "| SGD | epoch: 003 | loss: 1.35945 - acc: 0.5328 -- iter: 01000/49500\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m1.35079\u001b[0m\u001b[0m | time: 0.648s\n",
      "| SGD | epoch: 003 | loss: 1.35079 - acc: 0.5355 -- iter: 02000/49500\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m1.34962\u001b[0m\u001b[0m | time: 1.245s\n",
      "| SGD | epoch: 003 | loss: 1.34962 - acc: 0.5300 -- iter: 03000/49500\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m1.33683\u001b[0m\u001b[0m | time: 1.857s\n",
      "| SGD | epoch: 003 | loss: 1.33683 - acc: 0.5340 -- iter: 04000/49500\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m1.31968\u001b[0m\u001b[0m | time: 2.429s\n",
      "| SGD | epoch: 003 | loss: 1.31968 - acc: 0.5405 -- iter: 05000/49500\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m1.29758\u001b[0m\u001b[0m | time: 3.005s\n",
      "| SGD | epoch: 003 | loss: 1.29758 - acc: 0.5487 -- iter: 06000/49500\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m1.29163\u001b[0m\u001b[0m | time: 3.591s\n",
      "| SGD | epoch: 003 | loss: 1.29163 - acc: 0.5498 -- iter: 07000/49500\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m1.28995\u001b[0m\u001b[0m | time: 4.190s\n",
      "| SGD | epoch: 003 | loss: 1.28995 - acc: 0.5478 -- iter: 08000/49500\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m1.27269\u001b[0m\u001b[0m | time: 4.810s\n",
      "| SGD | epoch: 003 | loss: 1.27269 - acc: 0.5557 -- iter: 09000/49500\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m1.25114\u001b[0m\u001b[0m | time: 5.375s\n",
      "| SGD | epoch: 003 | loss: 1.25114 - acc: 0.5639 -- iter: 10000/49500\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m1.23856\u001b[0m\u001b[0m | time: 6.009s\n",
      "| SGD | epoch: 003 | loss: 1.23856 - acc: 0.5688 -- iter: 11000/49500\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m1.23883\u001b[0m\u001b[0m | time: 6.603s\n",
      "| SGD | epoch: 003 | loss: 1.23883 - acc: 0.5681 -- iter: 12000/49500\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m1.21330\u001b[0m\u001b[0m | time: 7.199s\n",
      "| SGD | epoch: 003 | loss: 1.21330 - acc: 0.5782 -- iter: 13000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m1.20054\u001b[0m\u001b[0m | time: 7.821s\n",
      "| SGD | epoch: 003 | loss: 1.20054 - acc: 0.5826 -- iter: 14000/49500\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m1.19137\u001b[0m\u001b[0m | time: 8.402s\n",
      "| SGD | epoch: 003 | loss: 1.19137 - acc: 0.5849 -- iter: 15000/49500\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m1.18678\u001b[0m\u001b[0m | time: 8.993s\n",
      "| SGD | epoch: 003 | loss: 1.18678 - acc: 0.5862 -- iter: 16000/49500\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m1.18427\u001b[0m\u001b[0m | time: 9.649s\n",
      "| SGD | epoch: 003 | loss: 1.18427 - acc: 0.5832 -- iter: 17000/49500\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m1.17549\u001b[0m\u001b[0m | time: 10.316s\n",
      "| SGD | epoch: 003 | loss: 1.17549 - acc: 0.5862 -- iter: 18000/49500\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m1.16597\u001b[0m\u001b[0m | time: 10.992s\n",
      "| SGD | epoch: 003 | loss: 1.16597 - acc: 0.5878 -- iter: 19000/49500\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m1.15012\u001b[0m\u001b[0m | time: 11.678s\n",
      "| SGD | epoch: 003 | loss: 1.15012 - acc: 0.5964 -- iter: 20000/49500\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m1.42319\u001b[0m\u001b[0m | time: 12.338s\n",
      "| SGD | epoch: 003 | loss: 1.42319 - acc: 0.5465 -- iter: 21000/49500\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m1.41775\u001b[0m\u001b[0m | time: 12.955s\n",
      "| SGD | epoch: 003 | loss: 1.41775 - acc: 0.5475 -- iter: 22000/49500\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m1.38739\u001b[0m\u001b[0m | time: 13.570s\n",
      "| SGD | epoch: 003 | loss: 1.38739 - acc: 0.5541 -- iter: 23000/49500\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m1.34392\u001b[0m\u001b[0m | time: 14.155s\n",
      "| SGD | epoch: 003 | loss: 1.34392 - acc: 0.5709 -- iter: 24000/49500\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m1.30463\u001b[0m\u001b[0m | time: 14.744s\n",
      "| SGD | epoch: 003 | loss: 1.30463 - acc: 0.5827 -- iter: 25000/49500\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m1.26864\u001b[0m\u001b[0m | time: 15.313s\n",
      "| SGD | epoch: 003 | loss: 1.26864 - acc: 0.5944 -- iter: 26000/49500\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m1.22349\u001b[0m\u001b[0m | time: 15.888s\n",
      "| SGD | epoch: 003 | loss: 1.22349 - acc: 0.6086 -- iter: 27000/49500\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m1.18666\u001b[0m\u001b[0m | time: 16.455s\n",
      "| SGD | epoch: 003 | loss: 1.18666 - acc: 0.6191 -- iter: 28000/49500\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m1.15220\u001b[0m\u001b[0m | time: 17.032s\n",
      "| SGD | epoch: 003 | loss: 1.15220 - acc: 0.6310 -- iter: 29000/49500\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m1.11735\u001b[0m\u001b[0m | time: 17.596s\n",
      "| SGD | epoch: 003 | loss: 1.11735 - acc: 0.6406 -- iter: 30000/49500\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m1.09182\u001b[0m\u001b[0m | time: 18.156s\n",
      "| SGD | epoch: 003 | loss: 1.09182 - acc: 0.6483 -- iter: 31000/49500\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m1.06979\u001b[0m\u001b[0m | time: 18.734s\n",
      "| SGD | epoch: 003 | loss: 1.06979 - acc: 0.6519 -- iter: 32000/49500\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m1.05708\u001b[0m\u001b[0m | time: 19.289s\n",
      "| SGD | epoch: 003 | loss: 1.05708 - acc: 0.6549 -- iter: 33000/49500\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m1.03485\u001b[0m\u001b[0m | time: 19.884s\n",
      "| SGD | epoch: 003 | loss: 1.03485 - acc: 0.6625 -- iter: 34000/49500\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m1.02186\u001b[0m\u001b[0m | time: 20.467s\n",
      "| SGD | epoch: 003 | loss: 1.02186 - acc: 0.6655 -- iter: 35000/49500\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m1.01033\u001b[0m\u001b[0m | time: 21.042s\n",
      "| SGD | epoch: 003 | loss: 1.01033 - acc: 0.6667 -- iter: 36000/49500\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m1.00840\u001b[0m\u001b[0m | time: 21.615s\n",
      "| SGD | epoch: 003 | loss: 1.00840 - acc: 0.6651 -- iter: 37000/49500\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m1.00530\u001b[0m\u001b[0m | time: 22.166s\n",
      "| SGD | epoch: 003 | loss: 1.00530 - acc: 0.6648 -- iter: 38000/49500\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m1.01772\u001b[0m\u001b[0m | time: 22.734s\n",
      "| SGD | epoch: 003 | loss: 1.01772 - acc: 0.6597 -- iter: 39000/49500\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m1.00524\u001b[0m\u001b[0m | time: 23.282s\n",
      "| SGD | epoch: 003 | loss: 1.00524 - acc: 0.6642 -- iter: 40000/49500\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.99879\u001b[0m\u001b[0m | time: 23.896s\n",
      "| SGD | epoch: 003 | loss: 0.99879 - acc: 0.6635 -- iter: 41000/49500\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.99520\u001b[0m\u001b[0m | time: 24.447s\n",
      "| SGD | epoch: 003 | loss: 0.99520 - acc: 0.6638 -- iter: 42000/49500\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.98329\u001b[0m\u001b[0m | time: 25.012s\n",
      "| SGD | epoch: 003 | loss: 0.98329 - acc: 0.6668 -- iter: 43000/49500\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.96345\u001b[0m\u001b[0m | time: 25.624s\n",
      "| SGD | epoch: 003 | loss: 0.96345 - acc: 0.6737 -- iter: 44000/49500\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.94417\u001b[0m\u001b[0m | time: 26.319s\n",
      "| SGD | epoch: 003 | loss: 0.94417 - acc: 0.6802 -- iter: 45000/49500\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.92363\u001b[0m\u001b[0m | time: 27.045s\n",
      "| SGD | epoch: 003 | loss: 0.92363 - acc: 0.6862 -- iter: 46000/49500\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.91798\u001b[0m\u001b[0m | time: 27.743s\n",
      "| SGD | epoch: 003 | loss: 0.91798 - acc: 0.6874 -- iter: 47000/49500\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.91145\u001b[0m\u001b[0m | time: 28.405s\n",
      "| SGD | epoch: 003 | loss: 0.91145 - acc: 0.6878 -- iter: 48000/49500\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.90372\u001b[0m\u001b[0m | time: 29.001s\n",
      "| SGD | epoch: 003 | loss: 0.90372 - acc: 0.6904 -- iter: 49000/49500\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.89094\u001b[0m\u001b[0m | time: 30.633s\n",
      "| SGD | epoch: 003 | loss: 0.89094 - acc: 0.6942 | val_loss: 0.75342 - val_acc: 0.7365 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.88182\u001b[0m\u001b[0m | time: 0.580s\n",
      "| SGD | epoch: 004 | loss: 0.88182 - acc: 0.6967 -- iter: 01000/49500\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.86598\u001b[0m\u001b[0m | time: 0.905s\n",
      "| SGD | epoch: 004 | loss: 0.86598 - acc: 0.7020 -- iter: 02000/49500\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.84486\u001b[0m\u001b[0m | time: 1.203s\n",
      "| SGD | epoch: 004 | loss: 0.84486 - acc: 0.7086 -- iter: 03000/49500\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.82057\u001b[0m\u001b[0m | time: 1.756s\n",
      "| SGD | epoch: 004 | loss: 0.82057 - acc: 0.7196 -- iter: 04000/49500\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.80813\u001b[0m\u001b[0m | time: 2.326s\n",
      "| SGD | epoch: 004 | loss: 0.80813 - acc: 0.7232 -- iter: 05000/49500\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.80371\u001b[0m\u001b[0m | time: 2.900s\n",
      "| SGD | epoch: 004 | loss: 0.80371 - acc: 0.7233 -- iter: 06000/49500\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.82653\u001b[0m\u001b[0m | time: 3.461s\n",
      "| SGD | epoch: 004 | loss: 0.82653 - acc: 0.7131 -- iter: 07000/49500\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.84882\u001b[0m\u001b[0m | time: 4.054s\n",
      "| SGD | epoch: 004 | loss: 0.84882 - acc: 0.7062 -- iter: 08000/49500\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.85567\u001b[0m\u001b[0m | time: 4.619s\n",
      "| SGD | epoch: 004 | loss: 0.85567 - acc: 0.7015 -- iter: 09000/49500\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.85430\u001b[0m\u001b[0m | time: 5.262s\n",
      "| SGD | epoch: 004 | loss: 0.85430 - acc: 0.7009 -- iter: 10000/49500\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.85678\u001b[0m\u001b[0m | time: 5.824s\n",
      "| SGD | epoch: 004 | loss: 0.85678 - acc: 0.7006 -- iter: 11000/49500\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.84975\u001b[0m\u001b[0m | time: 6.392s\n",
      "| SGD | epoch: 004 | loss: 0.84975 - acc: 0.7069 -- iter: 12000/49500\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.83997\u001b[0m\u001b[0m | time: 6.959s\n",
      "| SGD | epoch: 004 | loss: 0.83997 - acc: 0.7121 -- iter: 13000/49500\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.83958\u001b[0m\u001b[0m | time: 7.523s\n",
      "| SGD | epoch: 004 | loss: 0.83958 - acc: 0.7121 -- iter: 14000/49500\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.82952\u001b[0m\u001b[0m | time: 8.095s\n",
      "| SGD | epoch: 004 | loss: 0.82952 - acc: 0.7155 -- iter: 15000/49500\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.83659\u001b[0m\u001b[0m | time: 8.652s\n",
      "| SGD | epoch: 004 | loss: 0.83659 - acc: 0.7148 -- iter: 16000/49500\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.82748\u001b[0m\u001b[0m | time: 9.222s\n",
      "| SGD | epoch: 004 | loss: 0.82748 - acc: 0.7187 -- iter: 17000/49500\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.81505\u001b[0m\u001b[0m | time: 9.784s\n",
      "| SGD | epoch: 004 | loss: 0.81505 - acc: 0.7237 -- iter: 18000/49500\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.80114\u001b[0m\u001b[0m | time: 10.342s\n",
      "| SGD | epoch: 004 | loss: 0.80114 - acc: 0.7292 -- iter: 19000/49500\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.78458\u001b[0m\u001b[0m | time: 10.919s\n",
      "| SGD | epoch: 004 | loss: 0.78458 - acc: 0.7380 -- iter: 20000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.77087\u001b[0m\u001b[0m | time: 11.580s\n",
      "| SGD | epoch: 004 | loss: 0.77087 - acc: 0.7420 -- iter: 21000/49500\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.75990\u001b[0m\u001b[0m | time: 12.264s\n",
      "| SGD | epoch: 004 | loss: 0.75990 - acc: 0.7449 -- iter: 22000/49500\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.75390\u001b[0m\u001b[0m | time: 13.045s\n",
      "| SGD | epoch: 004 | loss: 0.75390 - acc: 0.7462 -- iter: 23000/49500\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.74818\u001b[0m\u001b[0m | time: 13.714s\n",
      "| SGD | epoch: 004 | loss: 0.74818 - acc: 0.7475 -- iter: 24000/49500\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.73509\u001b[0m\u001b[0m | time: 14.313s\n",
      "| SGD | epoch: 004 | loss: 0.73509 - acc: 0.7521 -- iter: 25000/49500\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.72493\u001b[0m\u001b[0m | time: 14.904s\n",
      "| SGD | epoch: 004 | loss: 0.72493 - acc: 0.7555 -- iter: 26000/49500\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.71136\u001b[0m\u001b[0m | time: 15.465s\n",
      "| SGD | epoch: 004 | loss: 0.71136 - acc: 0.7595 -- iter: 27000/49500\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.70261\u001b[0m\u001b[0m | time: 16.026s\n",
      "| SGD | epoch: 004 | loss: 0.70261 - acc: 0.7631 -- iter: 28000/49500\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.70006\u001b[0m\u001b[0m | time: 16.602s\n",
      "| SGD | epoch: 004 | loss: 0.70006 - acc: 0.7627 -- iter: 29000/49500\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.69359\u001b[0m\u001b[0m | time: 17.173s\n",
      "| SGD | epoch: 004 | loss: 0.69359 - acc: 0.7645 -- iter: 30000/49500\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.68770\u001b[0m\u001b[0m | time: 17.721s\n",
      "| SGD | epoch: 004 | loss: 0.68770 - acc: 0.7654 -- iter: 31000/49500\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.68515\u001b[0m\u001b[0m | time: 18.296s\n",
      "| SGD | epoch: 004 | loss: 0.68515 - acc: 0.7637 -- iter: 32000/49500\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.67401\u001b[0m\u001b[0m | time: 18.854s\n",
      "| SGD | epoch: 004 | loss: 0.67401 - acc: 0.7658 -- iter: 33000/49500\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.66434\u001b[0m\u001b[0m | time: 19.407s\n",
      "| SGD | epoch: 004 | loss: 0.66434 - acc: 0.7685 -- iter: 34000/49500\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.65575\u001b[0m\u001b[0m | time: 20.033s\n",
      "| SGD | epoch: 004 | loss: 0.65575 - acc: 0.7745 -- iter: 35000/49500\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.64385\u001b[0m\u001b[0m | time: 20.597s\n",
      "| SGD | epoch: 004 | loss: 0.64385 - acc: 0.7784 -- iter: 36000/49500\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.63282\u001b[0m\u001b[0m | time: 21.169s\n",
      "| SGD | epoch: 004 | loss: 0.63282 - acc: 0.7825 -- iter: 37000/49500\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.62205\u001b[0m\u001b[0m | time: 21.716s\n",
      "| SGD | epoch: 004 | loss: 0.62205 - acc: 0.7866 -- iter: 38000/49500\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.61015\u001b[0m\u001b[0m | time: 22.286s\n",
      "| SGD | epoch: 004 | loss: 0.61015 - acc: 0.7937 -- iter: 39000/49500\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.60230\u001b[0m\u001b[0m | time: 22.853s\n",
      "| SGD | epoch: 004 | loss: 0.60230 - acc: 0.7980 -- iter: 40000/49500\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.59625\u001b[0m\u001b[0m | time: 23.412s\n",
      "| SGD | epoch: 004 | loss: 0.59625 - acc: 0.8009 -- iter: 41000/49500\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.59119\u001b[0m\u001b[0m | time: 23.982s\n",
      "| SGD | epoch: 004 | loss: 0.59119 - acc: 0.8020 -- iter: 42000/49500\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.58486\u001b[0m\u001b[0m | time: 24.539s\n",
      "| SGD | epoch: 004 | loss: 0.58486 - acc: 0.8023 -- iter: 43000/49500\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.59250\u001b[0m\u001b[0m | time: 25.113s\n",
      "| SGD | epoch: 004 | loss: 0.59250 - acc: 0.7973 -- iter: 44000/49500\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.59085\u001b[0m\u001b[0m | time: 25.710s\n",
      "| SGD | epoch: 004 | loss: 0.59085 - acc: 0.7963 -- iter: 45000/49500\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.59141\u001b[0m\u001b[0m | time: 26.278s\n",
      "| SGD | epoch: 004 | loss: 0.59141 - acc: 0.7968 -- iter: 46000/49500\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.59884\u001b[0m\u001b[0m | time: 26.909s\n",
      "| SGD | epoch: 004 | loss: 0.59884 - acc: 0.7952 -- iter: 47000/49500\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.59526\u001b[0m\u001b[0m | time: 27.580s\n",
      "| SGD | epoch: 004 | loss: 0.59526 - acc: 0.7962 -- iter: 48000/49500\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.58873\u001b[0m\u001b[0m | time: 28.268s\n",
      "| SGD | epoch: 004 | loss: 0.58873 - acc: 0.7981 -- iter: 49000/49500\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.58112\u001b[0m\u001b[0m | time: 30.110s\n",
      "| SGD | epoch: 004 | loss: 0.58112 - acc: 0.8014 | val_loss: 0.56699 - val_acc: 0.8142 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.58046\u001b[0m\u001b[0m | time: 0.576s\n",
      "| SGD | epoch: 005 | loss: 0.58046 - acc: 0.8028 -- iter: 01000/49500\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.58033\u001b[0m\u001b[0m | time: 1.162s\n",
      "| SGD | epoch: 005 | loss: 0.58033 - acc: 0.8043 -- iter: 02000/49500\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.58156\u001b[0m\u001b[0m | time: 1.462s\n",
      "| SGD | epoch: 005 | loss: 0.58156 - acc: 0.8034 -- iter: 03000/49500\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.58903\u001b[0m\u001b[0m | time: 1.772s\n",
      "| SGD | epoch: 005 | loss: 0.58903 - acc: 0.8025 -- iter: 04000/49500\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.58444\u001b[0m\u001b[0m | time: 2.325s\n",
      "| SGD | epoch: 005 | loss: 0.58444 - acc: 0.8062 -- iter: 05000/49500\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.58024\u001b[0m\u001b[0m | time: 2.897s\n",
      "| SGD | epoch: 005 | loss: 0.58024 - acc: 0.8065 -- iter: 06000/49500\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.58109\u001b[0m\u001b[0m | time: 3.443s\n",
      "| SGD | epoch: 005 | loss: 0.58109 - acc: 0.8065 -- iter: 07000/49500\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.59265\u001b[0m\u001b[0m | time: 4.036s\n",
      "| SGD | epoch: 005 | loss: 0.59265 - acc: 0.8016 -- iter: 08000/49500\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.60817\u001b[0m\u001b[0m | time: 4.625s\n",
      "| SGD | epoch: 005 | loss: 0.60817 - acc: 0.7952 -- iter: 09000/49500\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.63588\u001b[0m\u001b[0m | time: 5.206s\n",
      "| SGD | epoch: 005 | loss: 0.63588 - acc: 0.7844 -- iter: 10000/49500\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.64668\u001b[0m\u001b[0m | time: 5.781s\n",
      "| SGD | epoch: 005 | loss: 0.64668 - acc: 0.7782 -- iter: 11000/49500\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.65937\u001b[0m\u001b[0m | time: 6.332s\n",
      "| SGD | epoch: 005 | loss: 0.65937 - acc: 0.7732 -- iter: 12000/49500\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.64876\u001b[0m\u001b[0m | time: 6.902s\n",
      "| SGD | epoch: 005 | loss: 0.64876 - acc: 0.7765 -- iter: 13000/49500\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.63764\u001b[0m\u001b[0m | time: 7.450s\n",
      "| SGD | epoch: 005 | loss: 0.63764 - acc: 0.7810 -- iter: 14000/49500\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.62451\u001b[0m\u001b[0m | time: 8.015s\n",
      "| SGD | epoch: 005 | loss: 0.62451 - acc: 0.7847 -- iter: 15000/49500\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.63470\u001b[0m\u001b[0m | time: 8.581s\n",
      "| SGD | epoch: 005 | loss: 0.63470 - acc: 0.7808 -- iter: 16000/49500\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.62406\u001b[0m\u001b[0m | time: 9.134s\n",
      "| SGD | epoch: 005 | loss: 0.62406 - acc: 0.7851 -- iter: 17000/49500\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.60959\u001b[0m\u001b[0m | time: 9.721s\n",
      "| SGD | epoch: 005 | loss: 0.60959 - acc: 0.7924 -- iter: 18000/49500\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.59124\u001b[0m\u001b[0m | time: 10.297s\n",
      "| SGD | epoch: 005 | loss: 0.59124 - acc: 0.8003 -- iter: 19000/49500\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.57041\u001b[0m\u001b[0m | time: 10.859s\n",
      "| SGD | epoch: 005 | loss: 0.57041 - acc: 0.8089 -- iter: 20000/49500\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.55373\u001b[0m\u001b[0m | time: 11.412s\n",
      "| SGD | epoch: 005 | loss: 0.55373 - acc: 0.8148 -- iter: 21000/49500\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.53927\u001b[0m\u001b[0m | time: 11.977s\n",
      "| SGD | epoch: 005 | loss: 0.53927 - acc: 0.8199 -- iter: 22000/49500\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m1.02919\u001b[0m\u001b[0m | time: 12.525s\n",
      "| SGD | epoch: 005 | loss: 1.02919 - acc: 0.7476 -- iter: 23000/49500\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m1.02502\u001b[0m\u001b[0m | time: 13.189s\n",
      "| SGD | epoch: 005 | loss: 1.02502 - acc: 0.7407 -- iter: 24000/49500\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.99560\u001b[0m\u001b[0m | time: 13.880s\n",
      "| SGD | epoch: 005 | loss: 0.99560 - acc: 0.7410 -- iter: 25000/49500\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.94899\u001b[0m\u001b[0m | time: 14.539s\n",
      "| SGD | epoch: 005 | loss: 0.94899 - acc: 0.7523 -- iter: 26000/49500\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.90550\u001b[0m\u001b[0m | time: 15.249s\n",
      "| SGD | epoch: 005 | loss: 0.90550 - acc: 0.7617 -- iter: 27000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.86053\u001b[0m\u001b[0m | time: 15.908s\n",
      "| SGD | epoch: 005 | loss: 0.86053 - acc: 0.7723 -- iter: 28000/49500\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.81600\u001b[0m\u001b[0m | time: 16.472s\n",
      "| SGD | epoch: 005 | loss: 0.81600 - acc: 0.7836 -- iter: 29000/49500\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.78014\u001b[0m\u001b[0m | time: 17.042s\n",
      "| SGD | epoch: 005 | loss: 0.78014 - acc: 0.7918 -- iter: 30000/49500\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.74078\u001b[0m\u001b[0m | time: 17.600s\n",
      "| SGD | epoch: 005 | loss: 0.74078 - acc: 0.8015 -- iter: 31000/49500\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.70526\u001b[0m\u001b[0m | time: 18.166s\n",
      "| SGD | epoch: 005 | loss: 0.70526 - acc: 0.8102 -- iter: 32000/49500\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.67502\u001b[0m\u001b[0m | time: 18.758s\n",
      "| SGD | epoch: 005 | loss: 0.67502 - acc: 0.8177 -- iter: 33000/49500\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.64883\u001b[0m\u001b[0m | time: 19.308s\n",
      "| SGD | epoch: 005 | loss: 0.64883 - acc: 0.8236 -- iter: 34000/49500\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.62384\u001b[0m\u001b[0m | time: 19.918s\n",
      "| SGD | epoch: 005 | loss: 0.62384 - acc: 0.8295 -- iter: 35000/49500\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.60358\u001b[0m\u001b[0m | time: 20.469s\n",
      "| SGD | epoch: 005 | loss: 0.60358 - acc: 0.8329 -- iter: 36000/49500\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.58805\u001b[0m\u001b[0m | time: 21.039s\n",
      "| SGD | epoch: 005 | loss: 0.58805 - acc: 0.8347 -- iter: 37000/49500\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.58465\u001b[0m\u001b[0m | time: 21.610s\n",
      "| SGD | epoch: 005 | loss: 0.58465 - acc: 0.8327 -- iter: 38000/49500\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.57745\u001b[0m\u001b[0m | time: 22.164s\n",
      "| SGD | epoch: 005 | loss: 0.57745 - acc: 0.8324 -- iter: 39000/49500\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.56795\u001b[0m\u001b[0m | time: 22.730s\n",
      "| SGD | epoch: 005 | loss: 0.56795 - acc: 0.8325 -- iter: 40000/49500\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.55831\u001b[0m\u001b[0m | time: 23.287s\n",
      "| SGD | epoch: 005 | loss: 0.55831 - acc: 0.8330 -- iter: 41000/49500\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.53697\u001b[0m\u001b[0m | time: 23.866s\n",
      "| SGD | epoch: 005 | loss: 0.53697 - acc: 0.8405 -- iter: 42000/49500\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.51286\u001b[0m\u001b[0m | time: 24.421s\n",
      "| SGD | epoch: 005 | loss: 0.51286 - acc: 0.8487 -- iter: 43000/49500\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.49591\u001b[0m\u001b[0m | time: 25.009s\n",
      "| SGD | epoch: 005 | loss: 0.49591 - acc: 0.8540 -- iter: 44000/49500\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.48243\u001b[0m\u001b[0m | time: 25.611s\n",
      "| SGD | epoch: 005 | loss: 0.48243 - acc: 0.8588 -- iter: 45000/49500\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.46765\u001b[0m\u001b[0m | time: 26.178s\n",
      "| SGD | epoch: 005 | loss: 0.46765 - acc: 0.8643 -- iter: 46000/49500\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.45353\u001b[0m\u001b[0m | time: 26.770s\n",
      "| SGD | epoch: 005 | loss: 0.45353 - acc: 0.8680 -- iter: 47000/49500\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.44193\u001b[0m\u001b[0m | time: 27.330s\n",
      "| SGD | epoch: 005 | loss: 0.44193 - acc: 0.8711 -- iter: 48000/49500\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.43972\u001b[0m\u001b[0m | time: 27.889s\n",
      "| SGD | epoch: 005 | loss: 0.43972 - acc: 0.8694 -- iter: 49000/49500\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.43771\u001b[0m\u001b[0m | time: 29.718s\n",
      "| SGD | epoch: 005 | loss: 0.43771 - acc: 0.8668 | val_loss: 0.46355 - val_acc: 0.8444 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.43958\u001b[0m\u001b[0m | time: 0.658s\n",
      "| SGD | epoch: 006 | loss: 0.43958 - acc: 0.8653 -- iter: 01000/49500\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.44103\u001b[0m\u001b[0m | time: 1.351s\n",
      "| SGD | epoch: 006 | loss: 0.44103 - acc: 0.8620 -- iter: 02000/49500\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.43252\u001b[0m\u001b[0m | time: 1.976s\n",
      "| SGD | epoch: 006 | loss: 0.43252 - acc: 0.8653 -- iter: 03000/49500\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.42823\u001b[0m\u001b[0m | time: 2.281s\n",
      "| SGD | epoch: 006 | loss: 0.42823 - acc: 0.8665 -- iter: 04000/49500\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.41685\u001b[0m\u001b[0m | time: 2.591s\n",
      "| SGD | epoch: 006 | loss: 0.41685 - acc: 0.8703 -- iter: 05000/49500\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.40250\u001b[0m\u001b[0m | time: 3.170s\n",
      "| SGD | epoch: 006 | loss: 0.40250 - acc: 0.8747 -- iter: 06000/49500\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.39343\u001b[0m\u001b[0m | time: 3.720s\n",
      "| SGD | epoch: 006 | loss: 0.39343 - acc: 0.8775 -- iter: 07000/49500\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.38777\u001b[0m\u001b[0m | time: 4.304s\n",
      "| SGD | epoch: 006 | loss: 0.38777 - acc: 0.8796 -- iter: 08000/49500\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.38243\u001b[0m\u001b[0m | time: 4.929s\n",
      "| SGD | epoch: 006 | loss: 0.38243 - acc: 0.8816 -- iter: 09000/49500\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.37344\u001b[0m\u001b[0m | time: 5.495s\n",
      "| SGD | epoch: 006 | loss: 0.37344 - acc: 0.8844 -- iter: 10000/49500\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.36818\u001b[0m\u001b[0m | time: 6.092s\n",
      "| SGD | epoch: 006 | loss: 0.36818 - acc: 0.8864 -- iter: 11000/49500\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.36046\u001b[0m\u001b[0m | time: 6.650s\n",
      "| SGD | epoch: 006 | loss: 0.36046 - acc: 0.8893 -- iter: 12000/49500\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.35738\u001b[0m\u001b[0m | time: 7.227s\n",
      "| SGD | epoch: 006 | loss: 0.35738 - acc: 0.8907 -- iter: 13000/49500\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.35165\u001b[0m\u001b[0m | time: 7.787s\n",
      "| SGD | epoch: 006 | loss: 0.35165 - acc: 0.8938 -- iter: 14000/49500\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.35045\u001b[0m\u001b[0m | time: 8.353s\n",
      "| SGD | epoch: 006 | loss: 0.35045 - acc: 0.8938 -- iter: 15000/49500\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.34625\u001b[0m\u001b[0m | time: 8.918s\n",
      "| SGD | epoch: 006 | loss: 0.34625 - acc: 0.8953 -- iter: 16000/49500\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.34628\u001b[0m\u001b[0m | time: 9.482s\n",
      "| SGD | epoch: 006 | loss: 0.34628 - acc: 0.8948 -- iter: 17000/49500\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.34343\u001b[0m\u001b[0m | time: 10.055s\n",
      "| SGD | epoch: 006 | loss: 0.34343 - acc: 0.8952 -- iter: 18000/49500\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.33615\u001b[0m\u001b[0m | time: 10.606s\n",
      "| SGD | epoch: 006 | loss: 0.33615 - acc: 0.8978 -- iter: 19000/49500\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.33244\u001b[0m\u001b[0m | time: 11.186s\n",
      "| SGD | epoch: 006 | loss: 0.33244 - acc: 0.8986 -- iter: 20000/49500\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.33184\u001b[0m\u001b[0m | time: 11.746s\n",
      "| SGD | epoch: 006 | loss: 0.33184 - acc: 0.8982 -- iter: 21000/49500\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.32954\u001b[0m\u001b[0m | time: 12.304s\n",
      "| SGD | epoch: 006 | loss: 0.32954 - acc: 0.8993 -- iter: 22000/49500\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.33403\u001b[0m\u001b[0m | time: 12.873s\n",
      "| SGD | epoch: 006 | loss: 0.33403 - acc: 0.8970 -- iter: 23000/49500\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.86868\u001b[0m\u001b[0m | time: 13.429s\n",
      "| SGD | epoch: 006 | loss: 0.86868 - acc: 0.8183 -- iter: 24000/49500\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.91543\u001b[0m\u001b[0m | time: 13.986s\n",
      "| SGD | epoch: 006 | loss: 0.91543 - acc: 0.7925 -- iter: 25000/49500\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.90190\u001b[0m\u001b[0m | time: 14.553s\n",
      "| SGD | epoch: 006 | loss: 0.90190 - acc: 0.7864 -- iter: 26000/49500\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.88365\u001b[0m\u001b[0m | time: 15.224s\n",
      "| SGD | epoch: 006 | loss: 0.88365 - acc: 0.7859 -- iter: 27000/49500\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.84309\u001b[0m\u001b[0m | time: 15.897s\n",
      "| SGD | epoch: 006 | loss: 0.84309 - acc: 0.7930 -- iter: 28000/49500\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.79229\u001b[0m\u001b[0m | time: 16.565s\n",
      "| SGD | epoch: 006 | loss: 0.79229 - acc: 0.8052 -- iter: 29000/49500\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.74374\u001b[0m\u001b[0m | time: 17.243s\n",
      "| SGD | epoch: 006 | loss: 0.74374 - acc: 0.8176 -- iter: 30000/49500\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.70319\u001b[0m\u001b[0m | time: 17.879s\n",
      "| SGD | epoch: 006 | loss: 0.70319 - acc: 0.8265 -- iter: 31000/49500\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.66090\u001b[0m\u001b[0m | time: 18.443s\n",
      "| SGD | epoch: 006 | loss: 0.66090 - acc: 0.8367 -- iter: 32000/49500\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.61917\u001b[0m\u001b[0m | time: 19.019s\n",
      "| SGD | epoch: 006 | loss: 0.61917 - acc: 0.8472 -- iter: 33000/49500\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.58752\u001b[0m\u001b[0m | time: 19.575s\n",
      "| SGD | epoch: 006 | loss: 0.58752 - acc: 0.8533 -- iter: 34000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.55751\u001b[0m\u001b[0m | time: 20.193s\n",
      "| SGD | epoch: 006 | loss: 0.55751 - acc: 0.8602 -- iter: 35000/49500\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.53002\u001b[0m\u001b[0m | time: 20.758s\n",
      "| SGD | epoch: 006 | loss: 0.53002 - acc: 0.8658 -- iter: 36000/49500\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.50463\u001b[0m\u001b[0m | time: 21.320s\n",
      "| SGD | epoch: 006 | loss: 0.50463 - acc: 0.8713 -- iter: 37000/49500\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.48040\u001b[0m\u001b[0m | time: 21.917s\n",
      "| SGD | epoch: 006 | loss: 0.48040 - acc: 0.8773 -- iter: 38000/49500\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.46043\u001b[0m\u001b[0m | time: 22.483s\n",
      "| SGD | epoch: 006 | loss: 0.46043 - acc: 0.8812 -- iter: 39000/49500\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.44020\u001b[0m\u001b[0m | time: 23.087s\n",
      "| SGD | epoch: 006 | loss: 0.44020 - acc: 0.8861 -- iter: 40000/49500\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.42320\u001b[0m\u001b[0m | time: 23.667s\n",
      "| SGD | epoch: 006 | loss: 0.42320 - acc: 0.8894 -- iter: 41000/49500\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.40713\u001b[0m\u001b[0m | time: 24.251s\n",
      "| SGD | epoch: 006 | loss: 0.40713 - acc: 0.8932 -- iter: 42000/49500\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.39087\u001b[0m\u001b[0m | time: 24.808s\n",
      "| SGD | epoch: 006 | loss: 0.39087 - acc: 0.8977 -- iter: 43000/49500\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.37457\u001b[0m\u001b[0m | time: 25.375s\n",
      "| SGD | epoch: 006 | loss: 0.37457 - acc: 0.9025 -- iter: 44000/49500\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.36596\u001b[0m\u001b[0m | time: 25.986s\n",
      "| SGD | epoch: 006 | loss: 0.36596 - acc: 0.9037 -- iter: 45000/49500\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.35490\u001b[0m\u001b[0m | time: 26.545s\n",
      "| SGD | epoch: 006 | loss: 0.35490 - acc: 0.9052 -- iter: 46000/49500\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.34564\u001b[0m\u001b[0m | time: 27.119s\n",
      "| SGD | epoch: 006 | loss: 0.34564 - acc: 0.9071 -- iter: 47000/49500\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.33629\u001b[0m\u001b[0m | time: 27.678s\n",
      "| SGD | epoch: 006 | loss: 0.33629 - acc: 0.9096 -- iter: 48000/49500\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.32904\u001b[0m\u001b[0m | time: 28.244s\n",
      "| SGD | epoch: 006 | loss: 0.32904 - acc: 0.9101 -- iter: 49000/49500\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.32526\u001b[0m\u001b[0m | time: 29.867s\n",
      "| SGD | epoch: 006 | loss: 0.32526 - acc: 0.9107 | val_loss: 0.25347 - val_acc: 0.9264 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.32248\u001b[0m\u001b[0m | time: 0.583s\n",
      "| SGD | epoch: 007 | loss: 0.32248 - acc: 0.9104 -- iter: 01000/49500\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.31512\u001b[0m\u001b[0m | time: 1.221s\n",
      "| SGD | epoch: 007 | loss: 0.31512 - acc: 0.9123 -- iter: 02000/49500\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.30656\u001b[0m\u001b[0m | time: 1.903s\n",
      "| SGD | epoch: 007 | loss: 0.30656 - acc: 0.9140 -- iter: 03000/49500\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.29802\u001b[0m\u001b[0m | time: 2.567s\n",
      "| SGD | epoch: 007 | loss: 0.29802 - acc: 0.9158 -- iter: 04000/49500\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.28924\u001b[0m\u001b[0m | time: 2.948s\n",
      "| SGD | epoch: 007 | loss: 0.28924 - acc: 0.9184 -- iter: 05000/49500\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.28157\u001b[0m\u001b[0m | time: 3.315s\n",
      "| SGD | epoch: 007 | loss: 0.28157 - acc: 0.9205 -- iter: 06000/49500\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.27330\u001b[0m\u001b[0m | time: 3.920s\n",
      "| SGD | epoch: 007 | loss: 0.27330 - acc: 0.9221 -- iter: 07000/49500\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.27101\u001b[0m\u001b[0m | time: 4.496s\n",
      "| SGD | epoch: 007 | loss: 0.27101 - acc: 0.9223 -- iter: 08000/49500\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.26953\u001b[0m\u001b[0m | time: 5.120s\n",
      "| SGD | epoch: 007 | loss: 0.26953 - acc: 0.9216 -- iter: 09000/49500\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.26879\u001b[0m\u001b[0m | time: 5.772s\n",
      "| SGD | epoch: 007 | loss: 0.26879 - acc: 0.9214 -- iter: 10000/49500\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.27055\u001b[0m\u001b[0m | time: 7.128s\n",
      "| SGD | epoch: 007 | loss: 0.27055 - acc: 0.9210 -- iter: 11000/49500\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.26873\u001b[0m\u001b[0m | time: 7.860s\n",
      "| SGD | epoch: 007 | loss: 0.26873 - acc: 0.9210 -- iter: 12000/49500\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.27863\u001b[0m\u001b[0m | time: 8.954s\n",
      "| SGD | epoch: 007 | loss: 0.27863 - acc: 0.9180 -- iter: 13000/49500\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.28169\u001b[0m\u001b[0m | time: 9.951s\n",
      "| SGD | epoch: 007 | loss: 0.28169 - acc: 0.9164 -- iter: 14000/49500\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.27719\u001b[0m\u001b[0m | time: 11.220s\n",
      "| SGD | epoch: 007 | loss: 0.27719 - acc: 0.9175 -- iter: 15000/49500\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.27614\u001b[0m\u001b[0m | time: 12.289s\n",
      "| SGD | epoch: 007 | loss: 0.27614 - acc: 0.9177 -- iter: 16000/49500\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.27064\u001b[0m\u001b[0m | time: 13.077s\n",
      "| SGD | epoch: 007 | loss: 0.27064 - acc: 0.9195 -- iter: 17000/49500\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.26480\u001b[0m\u001b[0m | time: 13.697s\n",
      "| SGD | epoch: 007 | loss: 0.26480 - acc: 0.9216 -- iter: 18000/49500\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.26400\u001b[0m\u001b[0m | time: 14.291s\n",
      "| SGD | epoch: 007 | loss: 0.26400 - acc: 0.9224 -- iter: 19000/49500\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.25826\u001b[0m\u001b[0m | time: 14.874s\n",
      "| SGD | epoch: 007 | loss: 0.25826 - acc: 0.9228 -- iter: 20000/49500\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.25256\u001b[0m\u001b[0m | time: 15.444s\n",
      "| SGD | epoch: 007 | loss: 0.25256 - acc: 0.9246 -- iter: 21000/49500\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.24895\u001b[0m\u001b[0m | time: 16.017s\n",
      "| SGD | epoch: 007 | loss: 0.24895 - acc: 0.9259 -- iter: 22000/49500\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.25446\u001b[0m\u001b[0m | time: 16.585s\n",
      "| SGD | epoch: 007 | loss: 0.25446 - acc: 0.9239 -- iter: 23000/49500\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.25955\u001b[0m\u001b[0m | time: 17.198s\n",
      "| SGD | epoch: 007 | loss: 0.25955 - acc: 0.9225 -- iter: 24000/49500\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.82877\u001b[0m\u001b[0m | time: 17.876s\n",
      "| SGD | epoch: 007 | loss: 0.82877 - acc: 0.8417 -- iter: 25000/49500\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.93633\u001b[0m\u001b[0m | time: 18.546s\n",
      "| SGD | epoch: 007 | loss: 0.93633 - acc: 0.7904 -- iter: 26000/49500\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.92851\u001b[0m\u001b[0m | time: 19.229s\n",
      "| SGD | epoch: 007 | loss: 0.92851 - acc: 0.7793 -- iter: 27000/49500\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.94047\u001b[0m\u001b[0m | time: 19.928s\n",
      "| SGD | epoch: 007 | loss: 0.94047 - acc: 0.7663 -- iter: 28000/49500\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.93908\u001b[0m\u001b[0m | time: 20.494s\n",
      "| SGD | epoch: 007 | loss: 0.93908 - acc: 0.7588 -- iter: 29000/49500\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.93385\u001b[0m\u001b[0m | time: 21.066s\n",
      "| SGD | epoch: 007 | loss: 0.93385 - acc: 0.7541 -- iter: 30000/49500\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.89308\u001b[0m\u001b[0m | time: 21.625s\n",
      "| SGD | epoch: 007 | loss: 0.89308 - acc: 0.7614 -- iter: 31000/49500\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.84999\u001b[0m\u001b[0m | time: 22.226s\n",
      "| SGD | epoch: 007 | loss: 0.84999 - acc: 0.7719 -- iter: 32000/49500\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.80717\u001b[0m\u001b[0m | time: 22.779s\n",
      "| SGD | epoch: 007 | loss: 0.80717 - acc: 0.7821 -- iter: 33000/49500\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.76230\u001b[0m\u001b[0m | time: 23.348s\n",
      "| SGD | epoch: 007 | loss: 0.76230 - acc: 0.7936 -- iter: 34000/49500\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.71854\u001b[0m\u001b[0m | time: 23.917s\n",
      "| SGD | epoch: 007 | loss: 0.71854 - acc: 0.8048 -- iter: 35000/49500\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.67759\u001b[0m\u001b[0m | time: 24.478s\n",
      "| SGD | epoch: 007 | loss: 0.67759 - acc: 0.8154 -- iter: 36000/49500\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.63648\u001b[0m\u001b[0m | time: 25.039s\n",
      "| SGD | epoch: 007 | loss: 0.63648 - acc: 0.8262 -- iter: 37000/49500\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.59993\u001b[0m\u001b[0m | time: 25.629s\n",
      "| SGD | epoch: 007 | loss: 0.59993 - acc: 0.8362 -- iter: 38000/49500\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.56393\u001b[0m\u001b[0m | time: 26.193s\n",
      "| SGD | epoch: 007 | loss: 0.56393 - acc: 0.8463 -- iter: 39000/49500\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.52945\u001b[0m\u001b[0m | time: 26.740s\n",
      "| SGD | epoch: 007 | loss: 0.52945 - acc: 0.8564 -- iter: 40000/49500\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.50411\u001b[0m\u001b[0m | time: 27.337s\n",
      "| SGD | epoch: 007 | loss: 0.50411 - acc: 0.8633 -- iter: 41000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.47934\u001b[0m\u001b[0m | time: 27.900s\n",
      "| SGD | epoch: 007 | loss: 0.47934 - acc: 0.8699 -- iter: 42000/49500\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.45393\u001b[0m\u001b[0m | time: 28.458s\n",
      "| SGD | epoch: 007 | loss: 0.45393 - acc: 0.8769 -- iter: 43000/49500\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.43116\u001b[0m\u001b[0m | time: 29.031s\n",
      "| SGD | epoch: 007 | loss: 0.43116 - acc: 0.8823 -- iter: 44000/49500\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.41078\u001b[0m\u001b[0m | time: 29.588s\n",
      "| SGD | epoch: 007 | loss: 0.41078 - acc: 0.8869 -- iter: 45000/49500\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.39234\u001b[0m\u001b[0m | time: 30.165s\n",
      "| SGD | epoch: 007 | loss: 0.39234 - acc: 0.8917 -- iter: 46000/49500\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.37741\u001b[0m\u001b[0m | time: 30.729s\n",
      "| SGD | epoch: 007 | loss: 0.37741 - acc: 0.8954 -- iter: 47000/49500\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.36176\u001b[0m\u001b[0m | time: 31.311s\n",
      "| SGD | epoch: 007 | loss: 0.36176 - acc: 0.8990 -- iter: 48000/49500\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.34744\u001b[0m\u001b[0m | time: 31.905s\n",
      "| SGD | epoch: 007 | loss: 0.34744 - acc: 0.9025 -- iter: 49000/49500\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.33185\u001b[0m\u001b[0m | time: 33.679s\n",
      "| SGD | epoch: 007 | loss: 0.33185 - acc: 0.9078 | val_loss: 0.21086 - val_acc: 0.9398 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.31781\u001b[0m\u001b[0m | time: 0.653s\n",
      "| SGD | epoch: 008 | loss: 0.31781 - acc: 0.9114 -- iter: 01000/49500\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.30767\u001b[0m\u001b[0m | time: 1.357s\n",
      "| SGD | epoch: 008 | loss: 0.30767 - acc: 0.9142 -- iter: 02000/49500\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.29643\u001b[0m\u001b[0m | time: 1.994s\n",
      "| SGD | epoch: 008 | loss: 0.29643 - acc: 0.9174 -- iter: 03000/49500\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.29144\u001b[0m\u001b[0m | time: 2.592s\n",
      "| SGD | epoch: 008 | loss: 0.29144 - acc: 0.9182 -- iter: 04000/49500\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.28395\u001b[0m\u001b[0m | time: 3.158s\n",
      "| SGD | epoch: 008 | loss: 0.28395 - acc: 0.9197 -- iter: 05000/49500\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.27755\u001b[0m\u001b[0m | time: 3.476s\n",
      "| SGD | epoch: 008 | loss: 0.27755 - acc: 0.9215 -- iter: 06000/49500\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.26961\u001b[0m\u001b[0m | time: 3.771s\n",
      "| SGD | epoch: 008 | loss: 0.26961 - acc: 0.9238 -- iter: 07000/49500\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.25801\u001b[0m\u001b[0m | time: 4.351s\n",
      "| SGD | epoch: 008 | loss: 0.25801 - acc: 0.9280 -- iter: 08000/49500\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.25397\u001b[0m\u001b[0m | time: 4.918s\n",
      "| SGD | epoch: 008 | loss: 0.25397 - acc: 0.9294 -- iter: 09000/49500\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.25049\u001b[0m\u001b[0m | time: 5.485s\n",
      "| SGD | epoch: 008 | loss: 0.25049 - acc: 0.9308 -- iter: 10000/49500\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.24977\u001b[0m\u001b[0m | time: 6.044s\n",
      "| SGD | epoch: 008 | loss: 0.24977 - acc: 0.9307 -- iter: 11000/49500\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.24704\u001b[0m\u001b[0m | time: 6.659s\n",
      "| SGD | epoch: 008 | loss: 0.24704 - acc: 0.9312 -- iter: 12000/49500\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.24603\u001b[0m\u001b[0m | time: 7.262s\n",
      "| SGD | epoch: 008 | loss: 0.24603 - acc: 0.9310 -- iter: 13000/49500\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.24952\u001b[0m\u001b[0m | time: 7.822s\n",
      "| SGD | epoch: 008 | loss: 0.24952 - acc: 0.9287 -- iter: 14000/49500\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.25309\u001b[0m\u001b[0m | time: 8.400s\n",
      "| SGD | epoch: 008 | loss: 0.25309 - acc: 0.9258 -- iter: 15000/49500\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.25336\u001b[0m\u001b[0m | time: 8.957s\n",
      "| SGD | epoch: 008 | loss: 0.25336 - acc: 0.9255 -- iter: 16000/49500\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.24732\u001b[0m\u001b[0m | time: 9.562s\n",
      "| SGD | epoch: 008 | loss: 0.24732 - acc: 0.9262 -- iter: 17000/49500\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.24173\u001b[0m\u001b[0m | time: 10.136s\n",
      "| SGD | epoch: 008 | loss: 0.24173 - acc: 0.9276 -- iter: 18000/49500\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.23353\u001b[0m\u001b[0m | time: 10.714s\n",
      "| SGD | epoch: 008 | loss: 0.23353 - acc: 0.9303 -- iter: 19000/49500\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.23193\u001b[0m\u001b[0m | time: 11.294s\n",
      "| SGD | epoch: 008 | loss: 0.23193 - acc: 0.9306 -- iter: 20000/49500\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.22533\u001b[0m\u001b[0m | time: 11.867s\n",
      "| SGD | epoch: 008 | loss: 0.22533 - acc: 0.9326 -- iter: 21000/49500\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.22043\u001b[0m\u001b[0m | time: 12.468s\n",
      "| SGD | epoch: 008 | loss: 0.22043 - acc: 0.9343 -- iter: 22000/49500\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.21560\u001b[0m\u001b[0m | time: 13.093s\n",
      "| SGD | epoch: 008 | loss: 0.21560 - acc: 0.9360 -- iter: 23000/49500\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.21309\u001b[0m\u001b[0m | time: 14.475s\n",
      "| SGD | epoch: 008 | loss: 0.21309 - acc: 0.9364 -- iter: 24000/49500\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.21118\u001b[0m\u001b[0m | time: 15.868s\n",
      "| SGD | epoch: 008 | loss: 0.21118 - acc: 0.9371 -- iter: 25000/49500\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.81760\u001b[0m\u001b[0m | time: 17.143s\n",
      "| SGD | epoch: 008 | loss: 0.81760 - acc: 0.8530 -- iter: 26000/49500\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.79567\u001b[0m\u001b[0m | time: 18.498s\n",
      "| SGD | epoch: 008 | loss: 0.79567 - acc: 0.8510 -- iter: 27000/49500\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.75538\u001b[0m\u001b[0m | time: 19.834s\n",
      "| SGD | epoch: 008 | loss: 0.75538 - acc: 0.8556 -- iter: 28000/49500\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.70483\u001b[0m\u001b[0m | time: 21.072s\n",
      "| SGD | epoch: 008 | loss: 0.70483 - acc: 0.8636 -- iter: 29000/49500\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.65593\u001b[0m\u001b[0m | time: 21.856s\n",
      "| SGD | epoch: 008 | loss: 0.65593 - acc: 0.8722 -- iter: 30000/49500\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.61143\u001b[0m\u001b[0m | time: 22.702s\n",
      "| SGD | epoch: 008 | loss: 0.61143 - acc: 0.8804 -- iter: 31000/49500\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.57069\u001b[0m\u001b[0m | time: 23.907s\n",
      "| SGD | epoch: 008 | loss: 0.57069 - acc: 0.8861 -- iter: 32000/49500\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.53065\u001b[0m\u001b[0m | time: 24.699s\n",
      "| SGD | epoch: 008 | loss: 0.53065 - acc: 0.8938 -- iter: 33000/49500\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.49654\u001b[0m\u001b[0m | time: 25.517s\n",
      "| SGD | epoch: 008 | loss: 0.49654 - acc: 0.8993 -- iter: 34000/49500\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.46799\u001b[0m\u001b[0m | time: 26.525s\n",
      "| SGD | epoch: 008 | loss: 0.46799 - acc: 0.9039 -- iter: 35000/49500\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.44383\u001b[0m\u001b[0m | time: 27.747s\n",
      "| SGD | epoch: 008 | loss: 0.44383 - acc: 0.9073 -- iter: 36000/49500\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.41826\u001b[0m\u001b[0m | time: 29.258s\n",
      "| SGD | epoch: 008 | loss: 0.41826 - acc: 0.9111 -- iter: 37000/49500\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.39714\u001b[0m\u001b[0m | time: 30.518s\n",
      "| SGD | epoch: 008 | loss: 0.39714 - acc: 0.9139 -- iter: 38000/49500\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.37441\u001b[0m\u001b[0m | time: 31.656s\n",
      "| SGD | epoch: 008 | loss: 0.37441 - acc: 0.9184 -- iter: 39000/49500\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.35588\u001b[0m\u001b[0m | time: 32.433s\n",
      "| SGD | epoch: 008 | loss: 0.35588 - acc: 0.9212 -- iter: 40000/49500\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.34166\u001b[0m\u001b[0m | time: 33.146s\n",
      "| SGD | epoch: 008 | loss: 0.34166 - acc: 0.9230 -- iter: 41000/49500\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.32838\u001b[0m\u001b[0m | time: 33.849s\n",
      "| SGD | epoch: 008 | loss: 0.32838 - acc: 0.9243 -- iter: 42000/49500\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.31474\u001b[0m\u001b[0m | time: 34.467s\n",
      "| SGD | epoch: 008 | loss: 0.31474 - acc: 0.9262 -- iter: 43000/49500\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.30409\u001b[0m\u001b[0m | time: 35.120s\n",
      "| SGD | epoch: 008 | loss: 0.30409 - acc: 0.9282 -- iter: 44000/49500\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.28907\u001b[0m\u001b[0m | time: 35.802s\n",
      "| SGD | epoch: 008 | loss: 0.28907 - acc: 0.9314 -- iter: 45000/49500\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.28117\u001b[0m\u001b[0m | time: 36.817s\n",
      "| SGD | epoch: 008 | loss: 0.28117 - acc: 0.9326 -- iter: 46000/49500\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.27048\u001b[0m\u001b[0m | time: 37.612s\n",
      "| SGD | epoch: 008 | loss: 0.27048 - acc: 0.9345 -- iter: 47000/49500\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.26374\u001b[0m\u001b[0m | time: 38.279s\n",
      "| SGD | epoch: 008 | loss: 0.26374 - acc: 0.9354 -- iter: 48000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.25677\u001b[0m\u001b[0m | time: 39.139s\n",
      "| SGD | epoch: 008 | loss: 0.25677 - acc: 0.9365 -- iter: 49000/49500\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.24467\u001b[0m\u001b[0m | time: 42.187s\n",
      "| SGD | epoch: 008 | loss: 0.24467 - acc: 0.9391 | val_loss: 0.17387 - val_acc: 0.9553 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.23867\u001b[0m\u001b[0m | time: 0.821s\n",
      "| SGD | epoch: 009 | loss: 0.23867 - acc: 0.9402 -- iter: 01000/49500\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.23084\u001b[0m\u001b[0m | time: 1.728s\n",
      "| SGD | epoch: 009 | loss: 0.23084 - acc: 0.9418 -- iter: 02000/49500\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.22622\u001b[0m\u001b[0m | time: 2.730s\n",
      "| SGD | epoch: 009 | loss: 0.22622 - acc: 0.9428 -- iter: 03000/49500\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.22532\u001b[0m\u001b[0m | time: 3.520s\n",
      "| SGD | epoch: 009 | loss: 0.22532 - acc: 0.9421 -- iter: 04000/49500\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.21788\u001b[0m\u001b[0m | time: 4.250s\n",
      "| SGD | epoch: 009 | loss: 0.21788 - acc: 0.9436 -- iter: 05000/49500\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.21330\u001b[0m\u001b[0m | time: 5.125s\n",
      "| SGD | epoch: 009 | loss: 0.21330 - acc: 0.9443 -- iter: 06000/49500\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.20751\u001b[0m\u001b[0m | time: 5.655s\n",
      "| SGD | epoch: 009 | loss: 0.20751 - acc: 0.9452 -- iter: 07000/49500\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.20268\u001b[0m\u001b[0m | time: 6.133s\n",
      "| SGD | epoch: 009 | loss: 0.20268 - acc: 0.9467 -- iter: 08000/49500\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.19657\u001b[0m\u001b[0m | time: 7.118s\n",
      "| SGD | epoch: 009 | loss: 0.19657 - acc: 0.9480 -- iter: 09000/49500\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.19381\u001b[0m\u001b[0m | time: 7.871s\n",
      "| SGD | epoch: 009 | loss: 0.19381 - acc: 0.9486 -- iter: 10000/49500\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.19155\u001b[0m\u001b[0m | time: 8.649s\n",
      "| SGD | epoch: 009 | loss: 0.19155 - acc: 0.9486 -- iter: 11000/49500\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.19006\u001b[0m\u001b[0m | time: 9.409s\n",
      "| SGD | epoch: 009 | loss: 0.19006 - acc: 0.9485 -- iter: 12000/49500\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.19275\u001b[0m\u001b[0m | time: 10.183s\n",
      "| SGD | epoch: 009 | loss: 0.19275 - acc: 0.9473 -- iter: 13000/49500\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.19569\u001b[0m\u001b[0m | time: 10.850s\n",
      "| SGD | epoch: 009 | loss: 0.19569 - acc: 0.9456 -- iter: 14000/49500\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.20263\u001b[0m\u001b[0m | time: 11.792s\n",
      "| SGD | epoch: 009 | loss: 0.20263 - acc: 0.9428 -- iter: 15000/49500\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.20804\u001b[0m\u001b[0m | time: 12.620s\n",
      "| SGD | epoch: 009 | loss: 0.20804 - acc: 0.9393 -- iter: 16000/49500\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.21087\u001b[0m\u001b[0m | time: 13.368s\n",
      "| SGD | epoch: 009 | loss: 0.21087 - acc: 0.9371 -- iter: 17000/49500\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.21039\u001b[0m\u001b[0m | time: 14.536s\n",
      "| SGD | epoch: 009 | loss: 0.21039 - acc: 0.9374 -- iter: 18000/49500\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.20986\u001b[0m\u001b[0m | time: 15.321s\n",
      "| SGD | epoch: 009 | loss: 0.20986 - acc: 0.9374 -- iter: 19000/49500\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.20645\u001b[0m\u001b[0m | time: 16.088s\n",
      "| SGD | epoch: 009 | loss: 0.20645 - acc: 0.9388 -- iter: 20000/49500\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.20312\u001b[0m\u001b[0m | time: 16.915s\n",
      "| SGD | epoch: 009 | loss: 0.20312 - acc: 0.9404 -- iter: 21000/49500\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.20105\u001b[0m\u001b[0m | time: 17.691s\n",
      "| SGD | epoch: 009 | loss: 0.20105 - acc: 0.9417 -- iter: 22000/49500\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.19741\u001b[0m\u001b[0m | time: 18.417s\n",
      "| SGD | epoch: 009 | loss: 0.19741 - acc: 0.9431 -- iter: 23000/49500\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.19156\u001b[0m\u001b[0m | time: 19.096s\n",
      "| SGD | epoch: 009 | loss: 0.19156 - acc: 0.9450 -- iter: 24000/49500\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.18644\u001b[0m\u001b[0m | time: 19.782s\n",
      "| SGD | epoch: 009 | loss: 0.18644 - acc: 0.9464 -- iter: 25000/49500\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.18549\u001b[0m\u001b[0m | time: 20.488s\n",
      "| SGD | epoch: 009 | loss: 0.18549 - acc: 0.9465 -- iter: 26000/49500\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.80441\u001b[0m\u001b[0m | time: 21.283s\n",
      "| SGD | epoch: 009 | loss: 0.80441 - acc: 0.8624 -- iter: 27000/49500\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.79520\u001b[0m\u001b[0m | time: 22.048s\n",
      "| SGD | epoch: 009 | loss: 0.79520 - acc: 0.8526 -- iter: 28000/49500\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.75682\u001b[0m\u001b[0m | time: 22.818s\n",
      "| SGD | epoch: 009 | loss: 0.75682 - acc: 0.8552 -- iter: 29000/49500\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.71034\u001b[0m\u001b[0m | time: 23.624s\n",
      "| SGD | epoch: 009 | loss: 0.71034 - acc: 0.8613 -- iter: 30000/49500\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.66222\u001b[0m\u001b[0m | time: 24.406s\n",
      "| SGD | epoch: 009 | loss: 0.66222 - acc: 0.8686 -- iter: 31000/49500\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.61833\u001b[0m\u001b[0m | time: 25.131s\n",
      "| SGD | epoch: 009 | loss: 0.61833 - acc: 0.8759 -- iter: 32000/49500\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.57494\u001b[0m\u001b[0m | time: 25.818s\n",
      "| SGD | epoch: 009 | loss: 0.57494 - acc: 0.8842 -- iter: 33000/49500\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.53519\u001b[0m\u001b[0m | time: 26.405s\n",
      "| SGD | epoch: 009 | loss: 0.53519 - acc: 0.8915 -- iter: 34000/49500\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.49965\u001b[0m\u001b[0m | time: 27.013s\n",
      "| SGD | epoch: 009 | loss: 0.49965 - acc: 0.8976 -- iter: 35000/49500\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.46958\u001b[0m\u001b[0m | time: 27.629s\n",
      "| SGD | epoch: 009 | loss: 0.46958 - acc: 0.9026 -- iter: 36000/49500\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.43721\u001b[0m\u001b[0m | time: 28.255s\n",
      "| SGD | epoch: 009 | loss: 0.43721 - acc: 0.9091 -- iter: 37000/49500\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.41283\u001b[0m\u001b[0m | time: 28.926s\n",
      "| SGD | epoch: 009 | loss: 0.41283 - acc: 0.9132 -- iter: 38000/49500\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.38746\u001b[0m\u001b[0m | time: 29.514s\n",
      "| SGD | epoch: 009 | loss: 0.38746 - acc: 0.9183 -- iter: 39000/49500\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.36555\u001b[0m\u001b[0m | time: 30.181s\n",
      "| SGD | epoch: 009 | loss: 0.36555 - acc: 0.9216 -- iter: 40000/49500\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.34688\u001b[0m\u001b[0m | time: 30.824s\n",
      "| SGD | epoch: 009 | loss: 0.34688 - acc: 0.9246 -- iter: 41000/49500\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.33007\u001b[0m\u001b[0m | time: 31.427s\n",
      "| SGD | epoch: 009 | loss: 0.33007 - acc: 0.9272 -- iter: 42000/49500\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.31238\u001b[0m\u001b[0m | time: 32.045s\n",
      "| SGD | epoch: 009 | loss: 0.31238 - acc: 0.9301 -- iter: 43000/49500\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.29697\u001b[0m\u001b[0m | time: 32.725s\n",
      "| SGD | epoch: 009 | loss: 0.29697 - acc: 0.9331 -- iter: 44000/49500\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.28291\u001b[0m\u001b[0m | time: 33.363s\n",
      "| SGD | epoch: 009 | loss: 0.28291 - acc: 0.9358 -- iter: 45000/49500\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.26866\u001b[0m\u001b[0m | time: 33.952s\n",
      "| SGD | epoch: 009 | loss: 0.26866 - acc: 0.9380 -- iter: 46000/49500\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.26256\u001b[0m\u001b[0m | time: 34.565s\n",
      "| SGD | epoch: 009 | loss: 0.26256 - acc: 0.9379 -- iter: 47000/49500\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.25259\u001b[0m\u001b[0m | time: 35.177s\n",
      "| SGD | epoch: 009 | loss: 0.25259 - acc: 0.9394 -- iter: 48000/49500\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.24116\u001b[0m\u001b[0m | time: 35.784s\n",
      "| SGD | epoch: 009 | loss: 0.24116 - acc: 0.9424 -- iter: 49000/49500\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.23485\u001b[0m\u001b[0m | time: 37.629s\n",
      "| SGD | epoch: 009 | loss: 0.23485 - acc: 0.9433 | val_loss: 0.16838 - val_acc: 0.9538 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.22761\u001b[0m\u001b[0m | time: 0.651s\n",
      "| SGD | epoch: 010 | loss: 0.22761 - acc: 0.9445 -- iter: 01000/49500\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.22176\u001b[0m\u001b[0m | time: 1.280s\n",
      "| SGD | epoch: 010 | loss: 0.22176 - acc: 0.9456 -- iter: 02000/49500\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.21375\u001b[0m\u001b[0m | time: 1.956s\n",
      "| SGD | epoch: 010 | loss: 0.21375 - acc: 0.9469 -- iter: 03000/49500\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.20739\u001b[0m\u001b[0m | time: 2.558s\n",
      "| SGD | epoch: 010 | loss: 0.20739 - acc: 0.9485 -- iter: 04000/49500\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.19947\u001b[0m\u001b[0m | time: 3.145s\n",
      "| SGD | epoch: 010 | loss: 0.19947 - acc: 0.9502 -- iter: 05000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.19336\u001b[0m\u001b[0m | time: 3.727s\n",
      "| SGD | epoch: 010 | loss: 0.19336 - acc: 0.9509 -- iter: 06000/49500\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.18919\u001b[0m\u001b[0m | time: 4.291s\n",
      "| SGD | epoch: 010 | loss: 0.18919 - acc: 0.9513 -- iter: 07000/49500\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.18545\u001b[0m\u001b[0m | time: 4.592s\n",
      "| SGD | epoch: 010 | loss: 0.18545 - acc: 0.9518 -- iter: 08000/49500\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.18420\u001b[0m\u001b[0m | time: 4.890s\n",
      "| SGD | epoch: 010 | loss: 0.18420 - acc: 0.9528 -- iter: 09000/49500\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.17955\u001b[0m\u001b[0m | time: 5.474s\n",
      "| SGD | epoch: 010 | loss: 0.17955 - acc: 0.9541 -- iter: 10000/49500\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.17879\u001b[0m\u001b[0m | time: 6.034s\n",
      "| SGD | epoch: 010 | loss: 0.17879 - acc: 0.9538 -- iter: 11000/49500\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.17442\u001b[0m\u001b[0m | time: 6.616s\n",
      "| SGD | epoch: 010 | loss: 0.17442 - acc: 0.9547 -- iter: 12000/49500\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.16947\u001b[0m\u001b[0m | time: 7.183s\n",
      "| SGD | epoch: 010 | loss: 0.16947 - acc: 0.9556 -- iter: 13000/49500\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.16789\u001b[0m\u001b[0m | time: 7.761s\n",
      "| SGD | epoch: 010 | loss: 0.16789 - acc: 0.9548 -- iter: 14000/49500\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.16447\u001b[0m\u001b[0m | time: 8.368s\n",
      "| SGD | epoch: 010 | loss: 0.16447 - acc: 0.9553 -- iter: 15000/49500\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.16421\u001b[0m\u001b[0m | time: 8.929s\n",
      "| SGD | epoch: 010 | loss: 0.16421 - acc: 0.9556 -- iter: 16000/49500\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.16543\u001b[0m\u001b[0m | time: 9.493s\n",
      "| SGD | epoch: 010 | loss: 0.16543 - acc: 0.9543 -- iter: 17000/49500\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.16229\u001b[0m\u001b[0m | time: 10.053s\n",
      "| SGD | epoch: 010 | loss: 0.16229 - acc: 0.9557 -- iter: 18000/49500\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.15989\u001b[0m\u001b[0m | time: 10.626s\n",
      "| SGD | epoch: 010 | loss: 0.15989 - acc: 0.9563 -- iter: 19000/49500\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.16030\u001b[0m\u001b[0m | time: 11.185s\n",
      "| SGD | epoch: 010 | loss: 0.16030 - acc: 0.9569 -- iter: 20000/49500\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.15826\u001b[0m\u001b[0m | time: 11.772s\n",
      "| SGD | epoch: 010 | loss: 0.15826 - acc: 0.9566 -- iter: 21000/49500\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.15312\u001b[0m\u001b[0m | time: 12.359s\n",
      "| SGD | epoch: 010 | loss: 0.15312 - acc: 0.9580 -- iter: 22000/49500\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.15006\u001b[0m\u001b[0m | time: 12.927s\n",
      "| SGD | epoch: 010 | loss: 0.15006 - acc: 0.9592 -- iter: 23000/49500\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.14682\u001b[0m\u001b[0m | time: 13.495s\n",
      "| SGD | epoch: 010 | loss: 0.14682 - acc: 0.9600 -- iter: 24000/49500\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.14544\u001b[0m\u001b[0m | time: 14.051s\n",
      "| SGD | epoch: 010 | loss: 0.14544 - acc: 0.9600 -- iter: 25000/49500\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.14416\u001b[0m\u001b[0m | time: 14.623s\n",
      "| SGD | epoch: 010 | loss: 0.14416 - acc: 0.9604 -- iter: 26000/49500\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.14233\u001b[0m\u001b[0m | time: 15.212s\n",
      "| SGD | epoch: 010 | loss: 0.14233 - acc: 0.9613 -- iter: 27000/49500\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.80376\u001b[0m\u001b[0m | time: 15.876s\n",
      "| SGD | epoch: 010 | loss: 0.80376 - acc: 0.8727 -- iter: 28000/49500\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.79002\u001b[0m\u001b[0m | time: 16.546s\n",
      "| SGD | epoch: 010 | loss: 0.79002 - acc: 0.8627 -- iter: 29000/49500\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.77976\u001b[0m\u001b[0m | time: 17.196s\n",
      "| SGD | epoch: 010 | loss: 0.77976 - acc: 0.8544 -- iter: 30000/49500\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.75441\u001b[0m\u001b[0m | time: 17.915s\n",
      "| SGD | epoch: 010 | loss: 0.75441 - acc: 0.8519 -- iter: 31000/49500\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.72458\u001b[0m\u001b[0m | time: 18.507s\n",
      "| SGD | epoch: 010 | loss: 0.72458 - acc: 0.8525 -- iter: 32000/49500\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.67495\u001b[0m\u001b[0m | time: 19.097s\n",
      "| SGD | epoch: 010 | loss: 0.67495 - acc: 0.8614 -- iter: 33000/49500\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.62940\u001b[0m\u001b[0m | time: 19.670s\n",
      "| SGD | epoch: 010 | loss: 0.62940 - acc: 0.8703 -- iter: 34000/49500\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.58847\u001b[0m\u001b[0m | time: 20.274s\n",
      "| SGD | epoch: 010 | loss: 0.58847 - acc: 0.8777 -- iter: 35000/49500\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.54914\u001b[0m\u001b[0m | time: 20.847s\n",
      "| SGD | epoch: 010 | loss: 0.54914 - acc: 0.8846 -- iter: 36000/49500\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.51197\u001b[0m\u001b[0m | time: 21.443s\n",
      "| SGD | epoch: 010 | loss: 0.51197 - acc: 0.8913 -- iter: 37000/49500\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.47557\u001b[0m\u001b[0m | time: 22.028s\n",
      "| SGD | epoch: 010 | loss: 0.47557 - acc: 0.8987 -- iter: 38000/49500\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.44329\u001b[0m\u001b[0m | time: 22.594s\n",
      "| SGD | epoch: 010 | loss: 0.44329 - acc: 0.9048 -- iter: 39000/49500\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.41421\u001b[0m\u001b[0m | time: 23.228s\n",
      "| SGD | epoch: 010 | loss: 0.41421 - acc: 0.9105 -- iter: 40000/49500\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.39097\u001b[0m\u001b[0m | time: 23.789s\n",
      "| SGD | epoch: 010 | loss: 0.39097 - acc: 0.9146 -- iter: 41000/49500\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.36641\u001b[0m\u001b[0m | time: 24.349s\n",
      "| SGD | epoch: 010 | loss: 0.36641 - acc: 0.9194 -- iter: 42000/49500\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.34774\u001b[0m\u001b[0m | time: 24.906s\n",
      "| SGD | epoch: 010 | loss: 0.34774 - acc: 0.9225 -- iter: 43000/49500\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.33049\u001b[0m\u001b[0m | time: 25.475s\n",
      "| SGD | epoch: 010 | loss: 0.33049 - acc: 0.9252 -- iter: 44000/49500\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.31593\u001b[0m\u001b[0m | time: 26.050s\n",
      "| SGD | epoch: 010 | loss: 0.31593 - acc: 0.9269 -- iter: 45000/49500\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.30096\u001b[0m\u001b[0m | time: 26.614s\n",
      "| SGD | epoch: 010 | loss: 0.30096 - acc: 0.9295 -- iter: 46000/49500\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.28524\u001b[0m\u001b[0m | time: 27.172s\n",
      "| SGD | epoch: 010 | loss: 0.28524 - acc: 0.9333 -- iter: 47000/49500\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.26934\u001b[0m\u001b[0m | time: 27.730s\n",
      "| SGD | epoch: 010 | loss: 0.26934 - acc: 0.9364 -- iter: 48000/49500\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.25742\u001b[0m\u001b[0m | time: 28.296s\n",
      "| SGD | epoch: 010 | loss: 0.25742 - acc: 0.9385 -- iter: 49000/49500\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.24725\u001b[0m\u001b[0m | time: 29.923s\n",
      "| SGD | epoch: 010 | loss: 0.24725 - acc: 0.9414 | val_loss: 0.15594 - val_acc: 0.9580 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.23575\u001b[0m\u001b[0m | time: 0.569s\n",
      "| SGD | epoch: 011 | loss: 0.23575 - acc: 0.9435 -- iter: 01000/49500\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.22684\u001b[0m\u001b[0m | time: 1.141s\n",
      "| SGD | epoch: 011 | loss: 0.22684 - acc: 0.9453 -- iter: 02000/49500\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.22003\u001b[0m\u001b[0m | time: 1.817s\n",
      "| SGD | epoch: 011 | loss: 0.22003 - acc: 0.9467 -- iter: 03000/49500\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.21258\u001b[0m\u001b[0m | time: 2.506s\n",
      "| SGD | epoch: 011 | loss: 0.21258 - acc: 0.9481 -- iter: 04000/49500\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.20476\u001b[0m\u001b[0m | time: 3.168s\n",
      "| SGD | epoch: 011 | loss: 0.20476 - acc: 0.9492 -- iter: 05000/49500\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.20036\u001b[0m\u001b[0m | time: 3.793s\n",
      "| SGD | epoch: 011 | loss: 0.20036 - acc: 0.9496 -- iter: 06000/49500\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.19469\u001b[0m\u001b[0m | time: 4.402s\n",
      "| SGD | epoch: 011 | loss: 0.19469 - acc: 0.9506 -- iter: 07000/49500\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.18855\u001b[0m\u001b[0m | time: 4.968s\n",
      "| SGD | epoch: 011 | loss: 0.18855 - acc: 0.9516 -- iter: 08000/49500\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.18301\u001b[0m\u001b[0m | time: 5.277s\n",
      "| SGD | epoch: 011 | loss: 0.18301 - acc: 0.9525 -- iter: 09000/49500\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.17851\u001b[0m\u001b[0m | time: 5.584s\n",
      "| SGD | epoch: 011 | loss: 0.17851 - acc: 0.9528 -- iter: 10000/49500\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.17187\u001b[0m\u001b[0m | time: 6.162s\n",
      "| SGD | epoch: 011 | loss: 0.17187 - acc: 0.9545 -- iter: 11000/49500\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.17046\u001b[0m\u001b[0m | time: 6.724s\n",
      "| SGD | epoch: 011 | loss: 0.17046 - acc: 0.9537 -- iter: 12000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.16875\u001b[0m\u001b[0m | time: 7.280s\n",
      "| SGD | epoch: 011 | loss: 0.16875 - acc: 0.9537 -- iter: 13000/49500\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.16630\u001b[0m\u001b[0m | time: 7.848s\n",
      "| SGD | epoch: 011 | loss: 0.16630 - acc: 0.9540 -- iter: 14000/49500\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.16226\u001b[0m\u001b[0m | time: 8.485s\n",
      "| SGD | epoch: 011 | loss: 0.16226 - acc: 0.9551 -- iter: 15000/49500\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.15697\u001b[0m\u001b[0m | time: 9.043s\n",
      "| SGD | epoch: 011 | loss: 0.15697 - acc: 0.9564 -- iter: 16000/49500\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.15437\u001b[0m\u001b[0m | time: 9.615s\n",
      "| SGD | epoch: 011 | loss: 0.15437 - acc: 0.9570 -- iter: 17000/49500\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.15503\u001b[0m\u001b[0m | time: 10.175s\n",
      "| SGD | epoch: 011 | loss: 0.15503 - acc: 0.9570 -- iter: 18000/49500\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.15389\u001b[0m\u001b[0m | time: 10.744s\n",
      "| SGD | epoch: 011 | loss: 0.15389 - acc: 0.9574 -- iter: 19000/49500\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.15111\u001b[0m\u001b[0m | time: 11.305s\n",
      "| SGD | epoch: 011 | loss: 0.15111 - acc: 0.9581 -- iter: 20000/49500\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.14928\u001b[0m\u001b[0m | time: 11.857s\n",
      "| SGD | epoch: 011 | loss: 0.14928 - acc: 0.9588 -- iter: 21000/49500\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.14595\u001b[0m\u001b[0m | time: 12.429s\n",
      "| SGD | epoch: 011 | loss: 0.14595 - acc: 0.9596 -- iter: 22000/49500\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.14455\u001b[0m\u001b[0m | time: 12.979s\n",
      "| SGD | epoch: 011 | loss: 0.14455 - acc: 0.9602 -- iter: 23000/49500\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.14149\u001b[0m\u001b[0m | time: 13.549s\n",
      "| SGD | epoch: 011 | loss: 0.14149 - acc: 0.9612 -- iter: 24000/49500\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.14170\u001b[0m\u001b[0m | time: 14.109s\n",
      "| SGD | epoch: 011 | loss: 0.14170 - acc: 0.9610 -- iter: 25000/49500\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.13942\u001b[0m\u001b[0m | time: 14.674s\n",
      "| SGD | epoch: 011 | loss: 0.13942 - acc: 0.9611 -- iter: 26000/49500\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.13886\u001b[0m\u001b[0m | time: 15.245s\n",
      "| SGD | epoch: 011 | loss: 0.13886 - acc: 0.9614 -- iter: 27000/49500\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.13815\u001b[0m\u001b[0m | time: 15.791s\n",
      "| SGD | epoch: 011 | loss: 0.13815 - acc: 0.9609 -- iter: 28000/49500\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.77414\u001b[0m\u001b[0m | time: 16.355s\n",
      "| SGD | epoch: 011 | loss: 0.77414 - acc: 0.8751 -- iter: 29000/49500\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.77234\u001b[0m\u001b[0m | time: 16.919s\n",
      "| SGD | epoch: 011 | loss: 0.77234 - acc: 0.8613 -- iter: 30000/49500\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.73182\u001b[0m\u001b[0m | time: 17.602s\n",
      "| SGD | epoch: 011 | loss: 0.73182 - acc: 0.8648 -- iter: 31000/49500\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.67877\u001b[0m\u001b[0m | time: 18.284s\n",
      "| SGD | epoch: 011 | loss: 0.67877 - acc: 0.8734 -- iter: 32000/49500\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.62639\u001b[0m\u001b[0m | time: 18.938s\n",
      "| SGD | epoch: 011 | loss: 0.62639 - acc: 0.8827 -- iter: 33000/49500\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.57826\u001b[0m\u001b[0m | time: 19.599s\n",
      "| SGD | epoch: 011 | loss: 0.57826 - acc: 0.8915 -- iter: 34000/49500\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.53856\u001b[0m\u001b[0m | time: 20.221s\n",
      "| SGD | epoch: 011 | loss: 0.53856 - acc: 0.8978 -- iter: 35000/49500\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.50006\u001b[0m\u001b[0m | time: 20.848s\n",
      "| SGD | epoch: 011 | loss: 0.50006 - acc: 0.9047 -- iter: 36000/49500\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.46340\u001b[0m\u001b[0m | time: 21.445s\n",
      "| SGD | epoch: 011 | loss: 0.46340 - acc: 0.9113 -- iter: 37000/49500\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.43239\u001b[0m\u001b[0m | time: 22.001s\n",
      "| SGD | epoch: 011 | loss: 0.43239 - acc: 0.9161 -- iter: 38000/49500\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.40286\u001b[0m\u001b[0m | time: 22.597s\n",
      "| SGD | epoch: 011 | loss: 0.40286 - acc: 0.9212 -- iter: 39000/49500\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.37922\u001b[0m\u001b[0m | time: 23.218s\n",
      "| SGD | epoch: 011 | loss: 0.37922 - acc: 0.9247 -- iter: 40000/49500\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.35599\u001b[0m\u001b[0m | time: 23.770s\n",
      "| SGD | epoch: 011 | loss: 0.35599 - acc: 0.9279 -- iter: 41000/49500\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.33578\u001b[0m\u001b[0m | time: 24.325s\n",
      "| SGD | epoch: 011 | loss: 0.33578 - acc: 0.9309 -- iter: 42000/49500\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.31784\u001b[0m\u001b[0m | time: 24.882s\n",
      "| SGD | epoch: 011 | loss: 0.31784 - acc: 0.9340 -- iter: 43000/49500\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.29967\u001b[0m\u001b[0m | time: 25.450s\n",
      "| SGD | epoch: 011 | loss: 0.29967 - acc: 0.9373 -- iter: 44000/49500\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.28253\u001b[0m\u001b[0m | time: 25.998s\n",
      "| SGD | epoch: 011 | loss: 0.28253 - acc: 0.9400 -- iter: 45000/49500\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.26888\u001b[0m\u001b[0m | time: 26.598s\n",
      "| SGD | epoch: 011 | loss: 0.26888 - acc: 0.9429 -- iter: 46000/49500\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.25408\u001b[0m\u001b[0m | time: 27.166s\n",
      "| SGD | epoch: 011 | loss: 0.25408 - acc: 0.9453 -- iter: 47000/49500\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.24070\u001b[0m\u001b[0m | time: 27.739s\n",
      "| SGD | epoch: 011 | loss: 0.24070 - acc: 0.9482 -- iter: 48000/49500\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.23074\u001b[0m\u001b[0m | time: 28.301s\n",
      "| SGD | epoch: 011 | loss: 0.23074 - acc: 0.9492 -- iter: 49000/49500\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.22430\u001b[0m\u001b[0m | time: 29.905s\n",
      "| SGD | epoch: 011 | loss: 0.22430 - acc: 0.9494 | val_loss: 0.13359 - val_acc: 0.9676 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.21382\u001b[0m\u001b[0m | time: 0.564s\n",
      "| SGD | epoch: 012 | loss: 0.21382 - acc: 0.9514 -- iter: 01000/49500\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.20716\u001b[0m\u001b[0m | time: 1.134s\n",
      "| SGD | epoch: 012 | loss: 0.20716 - acc: 0.9522 -- iter: 02000/49500\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.19827\u001b[0m\u001b[0m | time: 1.705s\n",
      "| SGD | epoch: 012 | loss: 0.19827 - acc: 0.9542 -- iter: 03000/49500\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.18955\u001b[0m\u001b[0m | time: 2.312s\n",
      "| SGD | epoch: 012 | loss: 0.18955 - acc: 0.9561 -- iter: 04000/49500\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.18230\u001b[0m\u001b[0m | time: 2.864s\n",
      "| SGD | epoch: 012 | loss: 0.18230 - acc: 0.9573 -- iter: 05000/49500\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.17848\u001b[0m\u001b[0m | time: 3.510s\n",
      "| SGD | epoch: 012 | loss: 0.17848 - acc: 0.9575 -- iter: 06000/49500\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.17200\u001b[0m\u001b[0m | time: 4.168s\n",
      "| SGD | epoch: 012 | loss: 0.17200 - acc: 0.9586 -- iter: 07000/49500\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.16807\u001b[0m\u001b[0m | time: 4.827s\n",
      "| SGD | epoch: 012 | loss: 0.16807 - acc: 0.9590 -- iter: 08000/49500\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.16297\u001b[0m\u001b[0m | time: 5.492s\n",
      "| SGD | epoch: 012 | loss: 0.16297 - acc: 0.9597 -- iter: 09000/49500\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.16036\u001b[0m\u001b[0m | time: 5.861s\n",
      "| SGD | epoch: 012 | loss: 0.16036 - acc: 0.9600 -- iter: 10000/49500\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.15655\u001b[0m\u001b[0m | time: 6.218s\n",
      "| SGD | epoch: 012 | loss: 0.15655 - acc: 0.9600 -- iter: 11000/49500\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.15013\u001b[0m\u001b[0m | time: 6.789s\n",
      "| SGD | epoch: 012 | loss: 0.15013 - acc: 0.9616 -- iter: 12000/49500\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.14937\u001b[0m\u001b[0m | time: 7.384s\n",
      "| SGD | epoch: 012 | loss: 0.14937 - acc: 0.9609 -- iter: 13000/49500\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.14615\u001b[0m\u001b[0m | time: 7.953s\n",
      "| SGD | epoch: 012 | loss: 0.14615 - acc: 0.9622 -- iter: 14000/49500\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.14462\u001b[0m\u001b[0m | time: 8.561s\n",
      "| SGD | epoch: 012 | loss: 0.14462 - acc: 0.9622 -- iter: 15000/49500\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.14325\u001b[0m\u001b[0m | time: 9.127s\n",
      "| SGD | epoch: 012 | loss: 0.14325 - acc: 0.9626 -- iter: 16000/49500\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.14306\u001b[0m\u001b[0m | time: 9.690s\n",
      "| SGD | epoch: 012 | loss: 0.14306 - acc: 0.9623 -- iter: 17000/49500\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.13968\u001b[0m\u001b[0m | time: 10.255s\n",
      "| SGD | epoch: 012 | loss: 0.13968 - acc: 0.9629 -- iter: 18000/49500\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.13957\u001b[0m\u001b[0m | time: 10.811s\n",
      "| SGD | epoch: 012 | loss: 0.13957 - acc: 0.9631 -- iter: 19000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.13747\u001b[0m\u001b[0m | time: 11.390s\n",
      "| SGD | epoch: 012 | loss: 0.13747 - acc: 0.9636 -- iter: 20000/49500\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.13571\u001b[0m\u001b[0m | time: 11.942s\n",
      "| SGD | epoch: 012 | loss: 0.13571 - acc: 0.9639 -- iter: 21000/49500\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.13546\u001b[0m\u001b[0m | time: 12.522s\n",
      "| SGD | epoch: 012 | loss: 0.13546 - acc: 0.9639 -- iter: 22000/49500\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.13394\u001b[0m\u001b[0m | time: 13.085s\n",
      "| SGD | epoch: 012 | loss: 0.13394 - acc: 0.9644 -- iter: 23000/49500\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.13156\u001b[0m\u001b[0m | time: 13.655s\n",
      "| SGD | epoch: 012 | loss: 0.13156 - acc: 0.9648 -- iter: 24000/49500\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.13103\u001b[0m\u001b[0m | time: 14.275s\n",
      "| SGD | epoch: 012 | loss: 0.13103 - acc: 0.9646 -- iter: 25000/49500\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.13077\u001b[0m\u001b[0m | time: 15.163s\n",
      "| SGD | epoch: 012 | loss: 0.13077 - acc: 0.9650 -- iter: 26000/49500\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.12780\u001b[0m\u001b[0m | time: 15.742s\n",
      "| SGD | epoch: 012 | loss: 0.12780 - acc: 0.9659 -- iter: 27000/49500\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.12810\u001b[0m\u001b[0m | time: 16.315s\n",
      "| SGD | epoch: 012 | loss: 0.12810 - acc: 0.9657 -- iter: 28000/49500\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.12662\u001b[0m\u001b[0m | time: 16.879s\n",
      "| SGD | epoch: 012 | loss: 0.12662 - acc: 0.9656 -- iter: 29000/49500\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.76972\u001b[0m\u001b[0m | time: 17.455s\n",
      "| SGD | epoch: 012 | loss: 0.76972 - acc: 0.8795 -- iter: 30000/49500\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.75149\u001b[0m\u001b[0m | time: 18.018s\n",
      "| SGD | epoch: 012 | loss: 0.75149 - acc: 0.8747 -- iter: 31000/49500\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.71973\u001b[0m\u001b[0m | time: 18.741s\n",
      "| SGD | epoch: 012 | loss: 0.71973 - acc: 0.8741 -- iter: 32000/49500\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.69933\u001b[0m\u001b[0m | time: 19.383s\n",
      "| SGD | epoch: 012 | loss: 0.69933 - acc: 0.8699 -- iter: 33000/49500\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.66238\u001b[0m\u001b[0m | time: 20.058s\n",
      "| SGD | epoch: 012 | loss: 0.66238 - acc: 0.8728 -- iter: 34000/49500\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.61828\u001b[0m\u001b[0m | time: 20.885s\n",
      "| SGD | epoch: 012 | loss: 0.61828 - acc: 0.8793 -- iter: 35000/49500\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.57255\u001b[0m\u001b[0m | time: 21.556s\n",
      "| SGD | epoch: 012 | loss: 0.57255 - acc: 0.8873 -- iter: 36000/49500\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.53026\u001b[0m\u001b[0m | time: 22.282s\n",
      "| SGD | epoch: 012 | loss: 0.53026 - acc: 0.8951 -- iter: 37000/49500\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.49202\u001b[0m\u001b[0m | time: 22.848s\n",
      "| SGD | epoch: 012 | loss: 0.49202 - acc: 0.9021 -- iter: 38000/49500\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.45753\u001b[0m\u001b[0m | time: 23.472s\n",
      "| SGD | epoch: 012 | loss: 0.45753 - acc: 0.9087 -- iter: 39000/49500\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.42341\u001b[0m\u001b[0m | time: 24.047s\n",
      "| SGD | epoch: 012 | loss: 0.42341 - acc: 0.9152 -- iter: 40000/49500\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.39349\u001b[0m\u001b[0m | time: 24.616s\n",
      "| SGD | epoch: 012 | loss: 0.39349 - acc: 0.9203 -- iter: 41000/49500\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.36839\u001b[0m\u001b[0m | time: 25.183s\n",
      "| SGD | epoch: 012 | loss: 0.36839 - acc: 0.9248 -- iter: 42000/49500\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.34401\u001b[0m\u001b[0m | time: 25.753s\n",
      "| SGD | epoch: 012 | loss: 0.34401 - acc: 0.9291 -- iter: 43000/49500\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.32317\u001b[0m\u001b[0m | time: 26.328s\n",
      "| SGD | epoch: 012 | loss: 0.32317 - acc: 0.9328 -- iter: 44000/49500\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.30272\u001b[0m\u001b[0m | time: 26.888s\n",
      "| SGD | epoch: 012 | loss: 0.30272 - acc: 0.9368 -- iter: 45000/49500\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.28293\u001b[0m\u001b[0m | time: 27.466s\n",
      "| SGD | epoch: 012 | loss: 0.28293 - acc: 0.9410 -- iter: 46000/49500\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.26689\u001b[0m\u001b[0m | time: 28.037s\n",
      "| SGD | epoch: 012 | loss: 0.26689 - acc: 0.9439 -- iter: 47000/49500\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.25240\u001b[0m\u001b[0m | time: 28.606s\n",
      "| SGD | epoch: 012 | loss: 0.25240 - acc: 0.9460 -- iter: 48000/49500\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.23870\u001b[0m\u001b[0m | time: 29.176s\n",
      "| SGD | epoch: 012 | loss: 0.23870 - acc: 0.9482 -- iter: 49000/49500\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.22915\u001b[0m\u001b[0m | time: 30.837s\n",
      "| SGD | epoch: 012 | loss: 0.22915 - acc: 0.9499 | val_loss: 0.12894 - val_acc: 0.9671 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.21583\u001b[0m\u001b[0m | time: 0.602s\n",
      "| SGD | epoch: 013 | loss: 0.21583 - acc: 0.9531 -- iter: 01000/49500\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.20681\u001b[0m\u001b[0m | time: 1.202s\n",
      "| SGD | epoch: 013 | loss: 0.20681 - acc: 0.9541 -- iter: 02000/49500\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.19859\u001b[0m\u001b[0m | time: 1.766s\n",
      "| SGD | epoch: 013 | loss: 0.19859 - acc: 0.9554 -- iter: 03000/49500\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.19018\u001b[0m\u001b[0m | time: 2.339s\n",
      "| SGD | epoch: 013 | loss: 0.19018 - acc: 0.9569 -- iter: 04000/49500\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.18361\u001b[0m\u001b[0m | time: 2.902s\n",
      "| SGD | epoch: 013 | loss: 0.18361 - acc: 0.9582 -- iter: 05000/49500\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.17853\u001b[0m\u001b[0m | time: 3.471s\n",
      "| SGD | epoch: 013 | loss: 0.17853 - acc: 0.9589 -- iter: 06000/49500\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.17512\u001b[0m\u001b[0m | time: 4.043s\n",
      "| SGD | epoch: 013 | loss: 0.17512 - acc: 0.9588 -- iter: 07000/49500\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.17118\u001b[0m\u001b[0m | time: 4.720s\n",
      "| SGD | epoch: 013 | loss: 0.17118 - acc: 0.9592 -- iter: 08000/49500\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.16628\u001b[0m\u001b[0m | time: 5.383s\n",
      "| SGD | epoch: 013 | loss: 0.16628 - acc: 0.9601 -- iter: 09000/49500\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.16319\u001b[0m\u001b[0m | time: 6.036s\n",
      "| SGD | epoch: 013 | loss: 0.16319 - acc: 0.9606 -- iter: 10000/49500\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.16313\u001b[0m\u001b[0m | time: 6.430s\n",
      "| SGD | epoch: 013 | loss: 0.16313 - acc: 0.9602 -- iter: 11000/49500\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.15868\u001b[0m\u001b[0m | time: 6.785s\n",
      "| SGD | epoch: 013 | loss: 0.15868 - acc: 0.9607 -- iter: 12000/49500\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.15121\u001b[0m\u001b[0m | time: 7.443s\n",
      "| SGD | epoch: 013 | loss: 0.15121 - acc: 0.9625 -- iter: 13000/49500\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.14803\u001b[0m\u001b[0m | time: 8.032s\n",
      "| SGD | epoch: 013 | loss: 0.14803 - acc: 0.9632 -- iter: 14000/49500\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.14841\u001b[0m\u001b[0m | time: 8.622s\n",
      "| SGD | epoch: 013 | loss: 0.14841 - acc: 0.9623 -- iter: 15000/49500\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.14563\u001b[0m\u001b[0m | time: 9.193s\n",
      "| SGD | epoch: 013 | loss: 0.14563 - acc: 0.9623 -- iter: 16000/49500\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.14476\u001b[0m\u001b[0m | time: 9.761s\n",
      "| SGD | epoch: 013 | loss: 0.14476 - acc: 0.9625 -- iter: 17000/49500\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.14212\u001b[0m\u001b[0m | time: 10.337s\n",
      "| SGD | epoch: 013 | loss: 0.14212 - acc: 0.9626 -- iter: 18000/49500\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.13915\u001b[0m\u001b[0m | time: 10.903s\n",
      "| SGD | epoch: 013 | loss: 0.13915 - acc: 0.9635 -- iter: 19000/49500\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.13806\u001b[0m\u001b[0m | time: 11.487s\n",
      "| SGD | epoch: 013 | loss: 0.13806 - acc: 0.9639 -- iter: 20000/49500\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.13746\u001b[0m\u001b[0m | time: 12.050s\n",
      "| SGD | epoch: 013 | loss: 0.13746 - acc: 0.9636 -- iter: 21000/49500\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.13549\u001b[0m\u001b[0m | time: 12.628s\n",
      "| SGD | epoch: 013 | loss: 0.13549 - acc: 0.9640 -- iter: 22000/49500\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.13385\u001b[0m\u001b[0m | time: 13.251s\n",
      "| SGD | epoch: 013 | loss: 0.13385 - acc: 0.9645 -- iter: 23000/49500\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.13073\u001b[0m\u001b[0m | time: 13.817s\n",
      "| SGD | epoch: 013 | loss: 0.13073 - acc: 0.9650 -- iter: 24000/49500\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.12822\u001b[0m\u001b[0m | time: 14.390s\n",
      "| SGD | epoch: 013 | loss: 0.12822 - acc: 0.9660 -- iter: 25000/49500\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.12504\u001b[0m\u001b[0m | time: 14.964s\n",
      "| SGD | epoch: 013 | loss: 0.12504 - acc: 0.9665 -- iter: 26000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.12561\u001b[0m\u001b[0m | time: 15.551s\n",
      "| SGD | epoch: 013 | loss: 0.12561 - acc: 0.9662 -- iter: 27000/49500\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.12324\u001b[0m\u001b[0m | time: 16.107s\n",
      "| SGD | epoch: 013 | loss: 0.12324 - acc: 0.9664 -- iter: 28000/49500\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.12023\u001b[0m\u001b[0m | time: 16.679s\n",
      "| SGD | epoch: 013 | loss: 0.12023 - acc: 0.9677 -- iter: 29000/49500\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.11851\u001b[0m\u001b[0m | time: 17.249s\n",
      "| SGD | epoch: 013 | loss: 0.11851 - acc: 0.9683 -- iter: 30000/49500\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.11646\u001b[0m\u001b[0m | time: 17.804s\n",
      "| SGD | epoch: 013 | loss: 0.11646 - acc: 0.9687 -- iter: 31000/49500\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.11621\u001b[0m\u001b[0m | time: 18.389s\n",
      "| SGD | epoch: 013 | loss: 0.11621 - acc: 0.9685 -- iter: 32000/49500\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.11433\u001b[0m\u001b[0m | time: 18.943s\n",
      "| SGD | epoch: 013 | loss: 0.11433 - acc: 0.9692 -- iter: 33000/49500\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.11255\u001b[0m\u001b[0m | time: 19.518s\n",
      "| SGD | epoch: 013 | loss: 0.11255 - acc: 0.9698 -- iter: 34000/49500\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.11163\u001b[0m\u001b[0m | time: 20.095s\n",
      "| SGD | epoch: 013 | loss: 0.11163 - acc: 0.9700 -- iter: 35000/49500\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.10966\u001b[0m\u001b[0m | time: 20.796s\n",
      "| SGD | epoch: 013 | loss: 0.10966 - acc: 0.9704 -- iter: 36000/49500\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.10701\u001b[0m\u001b[0m | time: 21.458s\n",
      "| SGD | epoch: 013 | loss: 0.10701 - acc: 0.9709 -- iter: 37000/49500\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.10377\u001b[0m\u001b[0m | time: 22.153s\n",
      "| SGD | epoch: 013 | loss: 0.10377 - acc: 0.9711 -- iter: 38000/49500\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.10240\u001b[0m\u001b[0m | time: 22.861s\n",
      "| SGD | epoch: 013 | loss: 0.10240 - acc: 0.9711 -- iter: 39000/49500\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.10150\u001b[0m\u001b[0m | time: 23.519s\n",
      "| SGD | epoch: 013 | loss: 0.10150 - acc: 0.9715 -- iter: 40000/49500\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.10297\u001b[0m\u001b[0m | time: 24.083s\n",
      "| SGD | epoch: 013 | loss: 0.10297 - acc: 0.9711 -- iter: 41000/49500\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.10396\u001b[0m\u001b[0m | time: 24.667s\n",
      "| SGD | epoch: 013 | loss: 0.10396 - acc: 0.9707 -- iter: 42000/49500\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.10486\u001b[0m\u001b[0m | time: 25.232s\n",
      "| SGD | epoch: 013 | loss: 0.10486 - acc: 0.9707 -- iter: 43000/49500\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.10413\u001b[0m\u001b[0m | time: 25.800s\n",
      "| SGD | epoch: 013 | loss: 0.10413 - acc: 0.9715 -- iter: 44000/49500\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.10448\u001b[0m\u001b[0m | time: 26.372s\n",
      "| SGD | epoch: 013 | loss: 0.10448 - acc: 0.9716 -- iter: 45000/49500\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.10427\u001b[0m\u001b[0m | time: 26.936s\n",
      "| SGD | epoch: 013 | loss: 0.10427 - acc: 0.9723 -- iter: 46000/49500\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.10453\u001b[0m\u001b[0m | time: 27.518s\n",
      "| SGD | epoch: 013 | loss: 0.10453 - acc: 0.9724 -- iter: 47000/49500\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.10598\u001b[0m\u001b[0m | time: 28.094s\n",
      "| SGD | epoch: 013 | loss: 0.10598 - acc: 0.9722 -- iter: 48000/49500\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.10712\u001b[0m\u001b[0m | time: 28.683s\n",
      "| SGD | epoch: 013 | loss: 0.10712 - acc: 0.9723 -- iter: 49000/49500\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.10703\u001b[0m\u001b[0m | time: 30.336s\n",
      "| SGD | epoch: 013 | loss: 0.10703 - acc: 0.9718 | val_loss: 0.11011 - val_acc: 0.9716 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.10548\u001b[0m\u001b[0m | time: 0.633s\n",
      "| SGD | epoch: 014 | loss: 0.10548 - acc: 0.9722 -- iter: 01000/49500\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.10575\u001b[0m\u001b[0m | time: 1.211s\n",
      "| SGD | epoch: 014 | loss: 0.10575 - acc: 0.9717 -- iter: 02000/49500\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.10203\u001b[0m\u001b[0m | time: 1.785s\n",
      "| SGD | epoch: 014 | loss: 0.10203 - acc: 0.9730 -- iter: 03000/49500\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.10560\u001b[0m\u001b[0m | time: 2.353s\n",
      "| SGD | epoch: 014 | loss: 0.10560 - acc: 0.9720 -- iter: 04000/49500\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.10657\u001b[0m\u001b[0m | time: 2.930s\n",
      "| SGD | epoch: 014 | loss: 0.10657 - acc: 0.9712 -- iter: 05000/49500\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.10689\u001b[0m\u001b[0m | time: 3.492s\n",
      "| SGD | epoch: 014 | loss: 0.10689 - acc: 0.9709 -- iter: 06000/49500\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.10666\u001b[0m\u001b[0m | time: 4.055s\n",
      "| SGD | epoch: 014 | loss: 0.10666 - acc: 0.9709 -- iter: 07000/49500\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.10319\u001b[0m\u001b[0m | time: 4.624s\n",
      "| SGD | epoch: 014 | loss: 0.10319 - acc: 0.9713 -- iter: 08000/49500\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.10185\u001b[0m\u001b[0m | time: 5.204s\n",
      "| SGD | epoch: 014 | loss: 0.10185 - acc: 0.9715 -- iter: 09000/49500\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.10132\u001b[0m\u001b[0m | time: 5.780s\n",
      "| SGD | epoch: 014 | loss: 0.10132 - acc: 0.9717 -- iter: 10000/49500\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.10351\u001b[0m\u001b[0m | time: 6.500s\n",
      "| SGD | epoch: 014 | loss: 0.10351 - acc: 0.9715 -- iter: 11000/49500\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.10499\u001b[0m\u001b[0m | time: 6.913s\n",
      "| SGD | epoch: 014 | loss: 0.10499 - acc: 0.9705 -- iter: 12000/49500\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.10554\u001b[0m\u001b[0m | time: 7.289s\n",
      "| SGD | epoch: 014 | loss: 0.10554 - acc: 0.9703 -- iter: 13000/49500\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.10301\u001b[0m\u001b[0m | time: 7.963s\n",
      "| SGD | epoch: 014 | loss: 0.10301 - acc: 0.9708 -- iter: 14000/49500\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.09923\u001b[0m\u001b[0m | time: 8.607s\n",
      "| SGD | epoch: 014 | loss: 0.09923 - acc: 0.9722 -- iter: 15000/49500\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.10074\u001b[0m\u001b[0m | time: 9.192s\n",
      "| SGD | epoch: 014 | loss: 0.10074 - acc: 0.9724 -- iter: 16000/49500\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.09852\u001b[0m\u001b[0m | time: 9.766s\n",
      "| SGD | epoch: 014 | loss: 0.09852 - acc: 0.9729 -- iter: 17000/49500\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.09823\u001b[0m\u001b[0m | time: 10.343s\n",
      "| SGD | epoch: 014 | loss: 0.09823 - acc: 0.9727 -- iter: 18000/49500\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.09799\u001b[0m\u001b[0m | time: 10.906s\n",
      "| SGD | epoch: 014 | loss: 0.09799 - acc: 0.9726 -- iter: 19000/49500\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.09790\u001b[0m\u001b[0m | time: 11.473s\n",
      "| SGD | epoch: 014 | loss: 0.09790 - acc: 0.9730 -- iter: 20000/49500\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.09882\u001b[0m\u001b[0m | time: 12.048s\n",
      "| SGD | epoch: 014 | loss: 0.09882 - acc: 0.9728 -- iter: 21000/49500\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.10278\u001b[0m\u001b[0m | time: 12.622s\n",
      "| SGD | epoch: 014 | loss: 0.10278 - acc: 0.9723 -- iter: 22000/49500\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.10897\u001b[0m\u001b[0m | time: 13.195s\n",
      "| SGD | epoch: 014 | loss: 0.10897 - acc: 0.9699 -- iter: 23000/49500\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.12639\u001b[0m\u001b[0m | time: 13.792s\n",
      "| SGD | epoch: 014 | loss: 0.12639 - acc: 0.9643 -- iter: 24000/49500\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.18182\u001b[0m\u001b[0m | time: 14.379s\n",
      "| SGD | epoch: 014 | loss: 0.18182 - acc: 0.9494 -- iter: 25000/49500\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.23695\u001b[0m\u001b[0m | time: 14.943s\n",
      "| SGD | epoch: 014 | loss: 0.23695 - acc: 0.9285 -- iter: 26000/49500\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.29230\u001b[0m\u001b[0m | time: 15.506s\n",
      "| SGD | epoch: 014 | loss: 0.29230 - acc: 0.9145 -- iter: 27000/49500\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.36895\u001b[0m\u001b[0m | time: 16.071s\n",
      "| SGD | epoch: 014 | loss: 0.36895 - acc: 0.8891 -- iter: 28000/49500\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.39266\u001b[0m\u001b[0m | time: 16.651s\n",
      "| SGD | epoch: 014 | loss: 0.39266 - acc: 0.8832 -- iter: 29000/49500\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.39729\u001b[0m\u001b[0m | time: 17.248s\n",
      "| SGD | epoch: 014 | loss: 0.39729 - acc: 0.8799 -- iter: 30000/49500\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.39436\u001b[0m\u001b[0m | time: 17.814s\n",
      "| SGD | epoch: 014 | loss: 0.39436 - acc: 0.8802 -- iter: 31000/49500\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.95291\u001b[0m\u001b[0m | time: 18.382s\n",
      "| SGD | epoch: 014 | loss: 0.95291 - acc: 0.8014 -- iter: 32000/49500\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m1.00164\u001b[0m\u001b[0m | time: 18.948s\n",
      "| SGD | epoch: 014 | loss: 1.00164 - acc: 0.7732 -- iter: 33000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.97247\u001b[0m\u001b[0m | time: 19.515s\n",
      "| SGD | epoch: 014 | loss: 0.97247 - acc: 0.7727 -- iter: 34000/49500\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.91808\u001b[0m\u001b[0m | time: 20.087s\n",
      "| SGD | epoch: 014 | loss: 0.91808 - acc: 0.7837 -- iter: 35000/49500\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.85932\u001b[0m\u001b[0m | time: 20.656s\n",
      "| SGD | epoch: 014 | loss: 0.85932 - acc: 0.7984 -- iter: 36000/49500\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.80045\u001b[0m\u001b[0m | time: 21.243s\n",
      "| SGD | epoch: 014 | loss: 0.80045 - acc: 0.8124 -- iter: 37000/49500\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.74642\u001b[0m\u001b[0m | time: 21.876s\n",
      "| SGD | epoch: 014 | loss: 0.74642 - acc: 0.8246 -- iter: 38000/49500\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.69329\u001b[0m\u001b[0m | time: 22.537s\n",
      "| SGD | epoch: 014 | loss: 0.69329 - acc: 0.8362 -- iter: 39000/49500\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.64603\u001b[0m\u001b[0m | time: 23.172s\n",
      "| SGD | epoch: 014 | loss: 0.64603 - acc: 0.8470 -- iter: 40000/49500\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.59978\u001b[0m\u001b[0m | time: 23.860s\n",
      "| SGD | epoch: 014 | loss: 0.59978 - acc: 0.8583 -- iter: 41000/49500\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.55852\u001b[0m\u001b[0m | time: 24.531s\n",
      "| SGD | epoch: 014 | loss: 0.55852 - acc: 0.8674 -- iter: 42000/49500\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.52015\u001b[0m\u001b[0m | time: 25.139s\n",
      "| SGD | epoch: 014 | loss: 0.52015 - acc: 0.8760 -- iter: 43000/49500\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.48332\u001b[0m\u001b[0m | time: 25.718s\n",
      "| SGD | epoch: 014 | loss: 0.48332 - acc: 0.8844 -- iter: 44000/49500\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.44856\u001b[0m\u001b[0m | time: 26.293s\n",
      "| SGD | epoch: 014 | loss: 0.44856 - acc: 0.8922 -- iter: 45000/49500\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.41809\u001b[0m\u001b[0m | time: 26.874s\n",
      "| SGD | epoch: 014 | loss: 0.41809 - acc: 0.8993 -- iter: 46000/49500\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.39097\u001b[0m\u001b[0m | time: 27.432s\n",
      "| SGD | epoch: 014 | loss: 0.39097 - acc: 0.9054 -- iter: 47000/49500\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.36716\u001b[0m\u001b[0m | time: 28.000s\n",
      "| SGD | epoch: 014 | loss: 0.36716 - acc: 0.9103 -- iter: 48000/49500\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.34252\u001b[0m\u001b[0m | time: 28.574s\n",
      "| SGD | epoch: 014 | loss: 0.34252 - acc: 0.9166 -- iter: 49000/49500\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.32335\u001b[0m\u001b[0m | time: 30.183s\n",
      "| SGD | epoch: 014 | loss: 0.32335 - acc: 0.9214 | val_loss: 0.14488 - val_acc: 0.9609 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.30382\u001b[0m\u001b[0m | time: 0.614s\n",
      "| SGD | epoch: 015 | loss: 0.30382 - acc: 0.9261 -- iter: 01000/49500\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.28849\u001b[0m\u001b[0m | time: 1.187s\n",
      "| SGD | epoch: 015 | loss: 0.28849 - acc: 0.9292 -- iter: 02000/49500\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.27668\u001b[0m\u001b[0m | time: 1.758s\n",
      "| SGD | epoch: 015 | loss: 0.27668 - acc: 0.9312 -- iter: 03000/49500\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.26215\u001b[0m\u001b[0m | time: 2.338s\n",
      "| SGD | epoch: 015 | loss: 0.26215 - acc: 0.9346 -- iter: 04000/49500\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.24947\u001b[0m\u001b[0m | time: 2.914s\n",
      "| SGD | epoch: 015 | loss: 0.24947 - acc: 0.9371 -- iter: 05000/49500\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.23742\u001b[0m\u001b[0m | time: 3.483s\n",
      "| SGD | epoch: 015 | loss: 0.23742 - acc: 0.9402 -- iter: 06000/49500\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.22720\u001b[0m\u001b[0m | time: 4.059s\n",
      "| SGD | epoch: 015 | loss: 0.22720 - acc: 0.9421 -- iter: 07000/49500\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.21763\u001b[0m\u001b[0m | time: 4.636s\n",
      "| SGD | epoch: 015 | loss: 0.21763 - acc: 0.9441 -- iter: 08000/49500\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.20856\u001b[0m\u001b[0m | time: 5.200s\n",
      "| SGD | epoch: 015 | loss: 0.20856 - acc: 0.9465 -- iter: 09000/49500\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.20269\u001b[0m\u001b[0m | time: 5.797s\n",
      "| SGD | epoch: 015 | loss: 0.20269 - acc: 0.9471 -- iter: 10000/49500\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.19421\u001b[0m\u001b[0m | time: 6.359s\n",
      "| SGD | epoch: 015 | loss: 0.19421 - acc: 0.9487 -- iter: 11000/49500\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.18734\u001b[0m\u001b[0m | time: 6.974s\n",
      "| SGD | epoch: 015 | loss: 0.18734 - acc: 0.9497 -- iter: 12000/49500\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.17866\u001b[0m\u001b[0m | time: 7.289s\n",
      "| SGD | epoch: 015 | loss: 0.17866 - acc: 0.9523 -- iter: 13000/49500\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.17414\u001b[0m\u001b[0m | time: 7.630s\n",
      "| SGD | epoch: 015 | loss: 0.17414 - acc: 0.9541 -- iter: 14000/49500\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.16683\u001b[0m\u001b[0m | time: 8.315s\n",
      "| SGD | epoch: 015 | loss: 0.16683 - acc: 0.9569 -- iter: 15000/49500\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.16120\u001b[0m\u001b[0m | time: 9.012s\n",
      "| SGD | epoch: 015 | loss: 0.16120 - acc: 0.9580 -- iter: 16000/49500\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.15752\u001b[0m\u001b[0m | time: 9.679s\n",
      "| SGD | epoch: 015 | loss: 0.15752 - acc: 0.9586 -- iter: 17000/49500\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.15163\u001b[0m\u001b[0m | time: 10.338s\n",
      "| SGD | epoch: 015 | loss: 0.15163 - acc: 0.9601 -- iter: 18000/49500\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.14822\u001b[0m\u001b[0m | time: 10.955s\n",
      "| SGD | epoch: 015 | loss: 0.14822 - acc: 0.9605 -- iter: 19000/49500\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.14436\u001b[0m\u001b[0m | time: 11.537s\n",
      "| SGD | epoch: 015 | loss: 0.14436 - acc: 0.9614 -- iter: 20000/49500\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.14324\u001b[0m\u001b[0m | time: 12.100s\n",
      "| SGD | epoch: 015 | loss: 0.14324 - acc: 0.9617 -- iter: 21000/49500\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.13831\u001b[0m\u001b[0m | time: 12.678s\n",
      "| SGD | epoch: 015 | loss: 0.13831 - acc: 0.9630 -- iter: 22000/49500\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.13392\u001b[0m\u001b[0m | time: 13.245s\n",
      "| SGD | epoch: 015 | loss: 0.13392 - acc: 0.9642 -- iter: 23000/49500\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.13039\u001b[0m\u001b[0m | time: 13.815s\n",
      "| SGD | epoch: 015 | loss: 0.13039 - acc: 0.9650 -- iter: 24000/49500\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.12918\u001b[0m\u001b[0m | time: 14.376s\n",
      "| SGD | epoch: 015 | loss: 0.12918 - acc: 0.9647 -- iter: 25000/49500\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.12944\u001b[0m\u001b[0m | time: 14.949s\n",
      "| SGD | epoch: 015 | loss: 0.12944 - acc: 0.9649 -- iter: 26000/49500\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.12860\u001b[0m\u001b[0m | time: 15.528s\n",
      "| SGD | epoch: 015 | loss: 0.12860 - acc: 0.9650 -- iter: 27000/49500\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.12770\u001b[0m\u001b[0m | time: 16.088s\n",
      "| SGD | epoch: 015 | loss: 0.12770 - acc: 0.9650 -- iter: 28000/49500\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.12603\u001b[0m\u001b[0m | time: 16.662s\n",
      "| SGD | epoch: 015 | loss: 0.12603 - acc: 0.9655 -- iter: 29000/49500\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.12322\u001b[0m\u001b[0m | time: 17.226s\n",
      "| SGD | epoch: 015 | loss: 0.12322 - acc: 0.9669 -- iter: 30000/49500\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.12346\u001b[0m\u001b[0m | time: 17.824s\n",
      "| SGD | epoch: 015 | loss: 0.12346 - acc: 0.9666 -- iter: 31000/49500\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.12264\u001b[0m\u001b[0m | time: 18.392s\n",
      "| SGD | epoch: 015 | loss: 0.12264 - acc: 0.9664 -- iter: 32000/49500\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.78303\u001b[0m\u001b[0m | time: 18.971s\n",
      "| SGD | epoch: 015 | loss: 0.78303 - acc: 0.8805 -- iter: 33000/49500\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.74414\u001b[0m\u001b[0m | time: 19.539s\n",
      "| SGD | epoch: 015 | loss: 0.74414 - acc: 0.8823 -- iter: 34000/49500\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.69303\u001b[0m\u001b[0m | time: 20.124s\n",
      "| SGD | epoch: 015 | loss: 0.69303 - acc: 0.8891 -- iter: 35000/49500\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.63759\u001b[0m\u001b[0m | time: 20.698s\n",
      "| SGD | epoch: 015 | loss: 0.63759 - acc: 0.8973 -- iter: 36000/49500\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.58682\u001b[0m\u001b[0m | time: 21.295s\n",
      "| SGD | epoch: 015 | loss: 0.58682 - acc: 0.9049 -- iter: 37000/49500\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.54153\u001b[0m\u001b[0m | time: 21.921s\n",
      "| SGD | epoch: 015 | loss: 0.54153 - acc: 0.9117 -- iter: 38000/49500\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.50043\u001b[0m\u001b[0m | time: 22.483s\n",
      "| SGD | epoch: 015 | loss: 0.50043 - acc: 0.9178 -- iter: 39000/49500\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.46345\u001b[0m\u001b[0m | time: 23.052s\n",
      "| SGD | epoch: 015 | loss: 0.46345 - acc: 0.9232 -- iter: 40000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.43003\u001b[0m\u001b[0m | time: 23.673s\n",
      "| SGD | epoch: 015 | loss: 0.43003 - acc: 0.9276 -- iter: 41000/49500\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.40008\u001b[0m\u001b[0m | time: 24.321s\n",
      "| SGD | epoch: 015 | loss: 0.40008 - acc: 0.9315 -- iter: 42000/49500\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.37094\u001b[0m\u001b[0m | time: 24.956s\n",
      "| SGD | epoch: 015 | loss: 0.37094 - acc: 0.9360 -- iter: 43000/49500\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.34481\u001b[0m\u001b[0m | time: 25.627s\n",
      "| SGD | epoch: 015 | loss: 0.34481 - acc: 0.9398 -- iter: 44000/49500\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.31950\u001b[0m\u001b[0m | time: 26.307s\n",
      "| SGD | epoch: 015 | loss: 0.31950 - acc: 0.9439 -- iter: 45000/49500\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.29874\u001b[0m\u001b[0m | time: 26.926s\n",
      "| SGD | epoch: 015 | loss: 0.29874 - acc: 0.9468 -- iter: 46000/49500\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.27913\u001b[0m\u001b[0m | time: 27.507s\n",
      "| SGD | epoch: 015 | loss: 0.27913 - acc: 0.9497 -- iter: 47000/49500\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.26137\u001b[0m\u001b[0m | time: 28.071s\n",
      "| SGD | epoch: 015 | loss: 0.26137 - acc: 0.9527 -- iter: 48000/49500\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.24414\u001b[0m\u001b[0m | time: 28.652s\n",
      "| SGD | epoch: 015 | loss: 0.24414 - acc: 0.9557 -- iter: 49000/49500\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.22986\u001b[0m\u001b[0m | time: 30.307s\n",
      "| SGD | epoch: 015 | loss: 0.22986 - acc: 0.9571 | val_loss: 0.11563 - val_acc: 0.9696 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.21685\u001b[0m\u001b[0m | time: 0.600s\n",
      "| SGD | epoch: 016 | loss: 0.21685 - acc: 0.9590 -- iter: 01000/49500\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.20395\u001b[0m\u001b[0m | time: 1.166s\n",
      "| SGD | epoch: 016 | loss: 0.20395 - acc: 0.9606 -- iter: 02000/49500\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.19332\u001b[0m\u001b[0m | time: 1.739s\n",
      "| SGD | epoch: 016 | loss: 0.19332 - acc: 0.9628 -- iter: 03000/49500\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.18317\u001b[0m\u001b[0m | time: 2.318s\n",
      "| SGD | epoch: 016 | loss: 0.18317 - acc: 0.9647 -- iter: 04000/49500\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.17657\u001b[0m\u001b[0m | time: 2.883s\n",
      "| SGD | epoch: 016 | loss: 0.17657 - acc: 0.9646 -- iter: 05000/49500\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.16924\u001b[0m\u001b[0m | time: 3.462s\n",
      "| SGD | epoch: 016 | loss: 0.16924 - acc: 0.9656 -- iter: 06000/49500\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.16245\u001b[0m\u001b[0m | time: 4.032s\n",
      "| SGD | epoch: 016 | loss: 0.16245 - acc: 0.9658 -- iter: 07000/49500\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.15926\u001b[0m\u001b[0m | time: 4.605s\n",
      "| SGD | epoch: 016 | loss: 0.15926 - acc: 0.9665 -- iter: 08000/49500\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.15208\u001b[0m\u001b[0m | time: 5.176s\n",
      "| SGD | epoch: 016 | loss: 0.15208 - acc: 0.9675 -- iter: 09000/49500\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.15088\u001b[0m\u001b[0m | time: 5.739s\n",
      "| SGD | epoch: 016 | loss: 0.15088 - acc: 0.9668 -- iter: 10000/49500\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.14536\u001b[0m\u001b[0m | time: 6.379s\n",
      "| SGD | epoch: 016 | loss: 0.14536 - acc: 0.9674 -- iter: 11000/49500\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.13949\u001b[0m\u001b[0m | time: 6.939s\n",
      "| SGD | epoch: 016 | loss: 0.13949 - acc: 0.9687 -- iter: 12000/49500\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.13745\u001b[0m\u001b[0m | time: 7.608s\n",
      "| SGD | epoch: 016 | loss: 0.13745 - acc: 0.9681 -- iter: 13000/49500\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.13377\u001b[0m\u001b[0m | time: 7.924s\n",
      "| SGD | epoch: 016 | loss: 0.13377 - acc: 0.9689 -- iter: 14000/49500\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.13438\u001b[0m\u001b[0m | time: 8.266s\n",
      "| SGD | epoch: 016 | loss: 0.13438 - acc: 0.9676 -- iter: 15000/49500\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.13227\u001b[0m\u001b[0m | time: 8.832s\n",
      "| SGD | epoch: 016 | loss: 0.13227 - acc: 0.9674 -- iter: 16000/49500\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.13133\u001b[0m\u001b[0m | time: 9.454s\n",
      "| SGD | epoch: 016 | loss: 0.13133 - acc: 0.9670 -- iter: 17000/49500\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.12923\u001b[0m\u001b[0m | time: 10.112s\n",
      "| SGD | epoch: 016 | loss: 0.12923 - acc: 0.9676 -- iter: 18000/49500\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.12644\u001b[0m\u001b[0m | time: 10.759s\n",
      "| SGD | epoch: 016 | loss: 0.12644 - acc: 0.9682 -- iter: 19000/49500\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.12391\u001b[0m\u001b[0m | time: 11.405s\n",
      "| SGD | epoch: 016 | loss: 0.12391 - acc: 0.9686 -- iter: 20000/49500\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.12134\u001b[0m\u001b[0m | time: 12.037s\n",
      "| SGD | epoch: 016 | loss: 0.12134 - acc: 0.9693 -- iter: 21000/49500\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.11953\u001b[0m\u001b[0m | time: 12.646s\n",
      "| SGD | epoch: 016 | loss: 0.11953 - acc: 0.9693 -- iter: 22000/49500\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.11843\u001b[0m\u001b[0m | time: 13.227s\n",
      "| SGD | epoch: 016 | loss: 0.11843 - acc: 0.9694 -- iter: 23000/49500\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.11682\u001b[0m\u001b[0m | time: 13.797s\n",
      "| SGD | epoch: 016 | loss: 0.11682 - acc: 0.9697 -- iter: 24000/49500\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.11728\u001b[0m\u001b[0m | time: 14.377s\n",
      "| SGD | epoch: 016 | loss: 0.11728 - acc: 0.9690 -- iter: 25000/49500\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.11519\u001b[0m\u001b[0m | time: 14.949s\n",
      "| SGD | epoch: 016 | loss: 0.11519 - acc: 0.9701 -- iter: 26000/49500\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.11443\u001b[0m\u001b[0m | time: 15.518s\n",
      "| SGD | epoch: 016 | loss: 0.11443 - acc: 0.9701 -- iter: 27000/49500\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.11366\u001b[0m\u001b[0m | time: 16.093s\n",
      "| SGD | epoch: 016 | loss: 0.11366 - acc: 0.9700 -- iter: 28000/49500\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.11150\u001b[0m\u001b[0m | time: 16.686s\n",
      "| SGD | epoch: 016 | loss: 0.11150 - acc: 0.9702 -- iter: 29000/49500\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.10913\u001b[0m\u001b[0m | time: 17.261s\n",
      "| SGD | epoch: 016 | loss: 0.10913 - acc: 0.9707 -- iter: 30000/49500\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.10988\u001b[0m\u001b[0m | time: 17.818s\n",
      "| SGD | epoch: 016 | loss: 0.10988 - acc: 0.9706 -- iter: 31000/49500\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.10844\u001b[0m\u001b[0m | time: 18.396s\n",
      "| SGD | epoch: 016 | loss: 0.10844 - acc: 0.9713 -- iter: 32000/49500\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.10552\u001b[0m\u001b[0m | time: 18.960s\n",
      "| SGD | epoch: 016 | loss: 0.10552 - acc: 0.9723 -- iter: 33000/49500\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.10372\u001b[0m\u001b[0m | time: 19.537s\n",
      "| SGD | epoch: 016 | loss: 0.10372 - acc: 0.9728 -- iter: 34000/49500\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.10471\u001b[0m\u001b[0m | time: 20.104s\n",
      "| SGD | epoch: 016 | loss: 0.10471 - acc: 0.9721 -- iter: 35000/49500\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.10227\u001b[0m\u001b[0m | time: 20.675s\n",
      "| SGD | epoch: 016 | loss: 0.10227 - acc: 0.9730 -- iter: 36000/49500\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.10314\u001b[0m\u001b[0m | time: 21.290s\n",
      "| SGD | epoch: 016 | loss: 0.10314 - acc: 0.9723 -- iter: 37000/49500\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.10120\u001b[0m\u001b[0m | time: 21.846s\n",
      "| SGD | epoch: 016 | loss: 0.10120 - acc: 0.9732 -- iter: 38000/49500\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.09872\u001b[0m\u001b[0m | time: 22.422s\n",
      "| SGD | epoch: 016 | loss: 0.09872 - acc: 0.9740 -- iter: 39000/49500\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.09591\u001b[0m\u001b[0m | time: 22.993s\n",
      "| SGD | epoch: 016 | loss: 0.09591 - acc: 0.9746 -- iter: 40000/49500\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.09298\u001b[0m\u001b[0m | time: 23.561s\n",
      "| SGD | epoch: 016 | loss: 0.09298 - acc: 0.9752 -- iter: 41000/49500\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.09240\u001b[0m\u001b[0m | time: 24.137s\n",
      "| SGD | epoch: 016 | loss: 0.09240 - acc: 0.9752 -- iter: 42000/49500\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.09145\u001b[0m\u001b[0m | time: 24.704s\n",
      "| SGD | epoch: 016 | loss: 0.09145 - acc: 0.9755 -- iter: 43000/49500\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.09243\u001b[0m\u001b[0m | time: 25.325s\n",
      "| SGD | epoch: 016 | loss: 0.09243 - acc: 0.9748 -- iter: 44000/49500\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.09357\u001b[0m\u001b[0m | time: 26.004s\n",
      "| SGD | epoch: 016 | loss: 0.09357 - acc: 0.9743 -- iter: 45000/49500\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.09272\u001b[0m\u001b[0m | time: 26.679s\n",
      "| SGD | epoch: 016 | loss: 0.09272 - acc: 0.9747 -- iter: 46000/49500\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.09335\u001b[0m\u001b[0m | time: 27.336s\n",
      "| SGD | epoch: 016 | loss: 0.09335 - acc: 0.9746 -- iter: 47000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.09431\u001b[0m\u001b[0m | time: 28.000s\n",
      "| SGD | epoch: 016 | loss: 0.09431 - acc: 0.9738 -- iter: 48000/49500\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.09398\u001b[0m\u001b[0m | time: 28.590s\n",
      "| SGD | epoch: 016 | loss: 0.09398 - acc: 0.9740 -- iter: 49000/49500\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.09592\u001b[0m\u001b[0m | time: 30.248s\n",
      "| SGD | epoch: 016 | loss: 0.09592 - acc: 0.9734 | val_loss: 0.10003 - val_acc: 0.9731 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.09367\u001b[0m\u001b[0m | time: 0.557s\n",
      "| SGD | epoch: 017 | loss: 0.09367 - acc: 0.9743 -- iter: 01000/49500\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.09162\u001b[0m\u001b[0m | time: 1.124s\n",
      "| SGD | epoch: 017 | loss: 0.09162 - acc: 0.9746 -- iter: 02000/49500\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.08972\u001b[0m\u001b[0m | time: 1.701s\n",
      "| SGD | epoch: 017 | loss: 0.08972 - acc: 0.9754 -- iter: 03000/49500\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.08875\u001b[0m\u001b[0m | time: 2.274s\n",
      "| SGD | epoch: 017 | loss: 0.08875 - acc: 0.9761 -- iter: 04000/49500\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.08824\u001b[0m\u001b[0m | time: 2.846s\n",
      "| SGD | epoch: 017 | loss: 0.08824 - acc: 0.9762 -- iter: 05000/49500\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.08906\u001b[0m\u001b[0m | time: 3.410s\n",
      "| SGD | epoch: 017 | loss: 0.08906 - acc: 0.9762 -- iter: 06000/49500\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.08829\u001b[0m\u001b[0m | time: 3.991s\n",
      "| SGD | epoch: 017 | loss: 0.08829 - acc: 0.9760 -- iter: 07000/49500\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.08961\u001b[0m\u001b[0m | time: 4.566s\n",
      "| SGD | epoch: 017 | loss: 0.08961 - acc: 0.9762 -- iter: 08000/49500\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.08759\u001b[0m\u001b[0m | time: 5.142s\n",
      "| SGD | epoch: 017 | loss: 0.08759 - acc: 0.9763 -- iter: 09000/49500\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.08725\u001b[0m\u001b[0m | time: 5.701s\n",
      "| SGD | epoch: 017 | loss: 0.08725 - acc: 0.9765 -- iter: 10000/49500\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.08547\u001b[0m\u001b[0m | time: 6.349s\n",
      "| SGD | epoch: 017 | loss: 0.08547 - acc: 0.9771 -- iter: 11000/49500\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.08808\u001b[0m\u001b[0m | time: 6.924s\n",
      "| SGD | epoch: 017 | loss: 0.08808 - acc: 0.9762 -- iter: 12000/49500\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.08532\u001b[0m\u001b[0m | time: 7.485s\n",
      "| SGD | epoch: 017 | loss: 0.08532 - acc: 0.9770 -- iter: 13000/49500\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.08482\u001b[0m\u001b[0m | time: 8.047s\n",
      "| SGD | epoch: 017 | loss: 0.08482 - acc: 0.9770 -- iter: 14000/49500\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.08523\u001b[0m\u001b[0m | time: 8.361s\n",
      "| SGD | epoch: 017 | loss: 0.08523 - acc: 0.9767 -- iter: 15000/49500\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.08444\u001b[0m\u001b[0m | time: 8.669s\n",
      "| SGD | epoch: 017 | loss: 0.08444 - acc: 0.9768 -- iter: 16000/49500\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.08193\u001b[0m\u001b[0m | time: 9.249s\n",
      "| SGD | epoch: 017 | loss: 0.08193 - acc: 0.9777 -- iter: 17000/49500\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.08576\u001b[0m\u001b[0m | time: 9.829s\n",
      "| SGD | epoch: 017 | loss: 0.08576 - acc: 0.9762 -- iter: 18000/49500\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.08625\u001b[0m\u001b[0m | time: 10.400s\n",
      "| SGD | epoch: 017 | loss: 0.08625 - acc: 0.9759 -- iter: 19000/49500\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.08451\u001b[0m\u001b[0m | time: 10.981s\n",
      "| SGD | epoch: 017 | loss: 0.08451 - acc: 0.9768 -- iter: 20000/49500\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.08500\u001b[0m\u001b[0m | time: 11.646s\n",
      "| SGD | epoch: 017 | loss: 0.08500 - acc: 0.9769 -- iter: 21000/49500\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.08595\u001b[0m\u001b[0m | time: 12.340s\n",
      "| SGD | epoch: 017 | loss: 0.08595 - acc: 0.9761 -- iter: 22000/49500\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.08488\u001b[0m\u001b[0m | time: 13.021s\n",
      "| SGD | epoch: 017 | loss: 0.08488 - acc: 0.9762 -- iter: 23000/49500\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.08206\u001b[0m\u001b[0m | time: 13.697s\n",
      "| SGD | epoch: 017 | loss: 0.08206 - acc: 0.9772 -- iter: 24000/49500\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.08191\u001b[0m\u001b[0m | time: 14.326s\n",
      "| SGD | epoch: 017 | loss: 0.08191 - acc: 0.9775 -- iter: 25000/49500\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.08440\u001b[0m\u001b[0m | time: 14.925s\n",
      "| SGD | epoch: 017 | loss: 0.08440 - acc: 0.9767 -- iter: 26000/49500\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.08347\u001b[0m\u001b[0m | time: 15.574s\n",
      "| SGD | epoch: 017 | loss: 0.08347 - acc: 0.9761 -- iter: 27000/49500\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.08426\u001b[0m\u001b[0m | time: 16.159s\n",
      "| SGD | epoch: 017 | loss: 0.08426 - acc: 0.9756 -- iter: 28000/49500\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.08427\u001b[0m\u001b[0m | time: 16.732s\n",
      "| SGD | epoch: 017 | loss: 0.08427 - acc: 0.9758 -- iter: 29000/49500\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.08256\u001b[0m\u001b[0m | time: 17.310s\n",
      "| SGD | epoch: 017 | loss: 0.08256 - acc: 0.9763 -- iter: 30000/49500\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.08233\u001b[0m\u001b[0m | time: 17.904s\n",
      "| SGD | epoch: 017 | loss: 0.08233 - acc: 0.9765 -- iter: 31000/49500\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.08255\u001b[0m\u001b[0m | time: 18.468s\n",
      "| SGD | epoch: 017 | loss: 0.08255 - acc: 0.9762 -- iter: 32000/49500\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.08273\u001b[0m\u001b[0m | time: 19.040s\n",
      "| SGD | epoch: 017 | loss: 0.08273 - acc: 0.9766 -- iter: 33000/49500\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.08318\u001b[0m\u001b[0m | time: 19.616s\n",
      "| SGD | epoch: 017 | loss: 0.08318 - acc: 0.9766 -- iter: 34000/49500\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.77686\u001b[0m\u001b[0m | time: 20.228s\n",
      "| SGD | epoch: 017 | loss: 0.77686 - acc: 0.8900 -- iter: 35000/49500\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.78947\u001b[0m\u001b[0m | time: 20.841s\n",
      "| SGD | epoch: 017 | loss: 0.78947 - acc: 0.8706 -- iter: 36000/49500\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.75936\u001b[0m\u001b[0m | time: 21.418s\n",
      "| SGD | epoch: 017 | loss: 0.75936 - acc: 0.8679 -- iter: 37000/49500\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.71718\u001b[0m\u001b[0m | time: 21.991s\n",
      "| SGD | epoch: 017 | loss: 0.71718 - acc: 0.8708 -- iter: 38000/49500\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.68283\u001b[0m\u001b[0m | time: 22.556s\n",
      "| SGD | epoch: 017 | loss: 0.68283 - acc: 0.8735 -- iter: 39000/49500\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.62902\u001b[0m\u001b[0m | time: 23.123s\n",
      "| SGD | epoch: 017 | loss: 0.62902 - acc: 0.8821 -- iter: 40000/49500\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.57943\u001b[0m\u001b[0m | time: 23.695s\n",
      "| SGD | epoch: 017 | loss: 0.57943 - acc: 0.8906 -- iter: 41000/49500\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.53389\u001b[0m\u001b[0m | time: 24.264s\n",
      "| SGD | epoch: 017 | loss: 0.53389 - acc: 0.8986 -- iter: 42000/49500\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.49127\u001b[0m\u001b[0m | time: 24.847s\n",
      "| SGD | epoch: 017 | loss: 0.49127 - acc: 0.9060 -- iter: 43000/49500\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.45168\u001b[0m\u001b[0m | time: 25.414s\n",
      "| SGD | epoch: 017 | loss: 0.45168 - acc: 0.9136 -- iter: 44000/49500\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.41626\u001b[0m\u001b[0m | time: 25.993s\n",
      "| SGD | epoch: 017 | loss: 0.41626 - acc: 0.9201 -- iter: 45000/49500\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.38892\u001b[0m\u001b[0m | time: 26.566s\n",
      "| SGD | epoch: 017 | loss: 0.38892 - acc: 0.9239 -- iter: 46000/49500\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.36001\u001b[0m\u001b[0m | time: 27.216s\n",
      "| SGD | epoch: 017 | loss: 0.36001 - acc: 0.9292 -- iter: 47000/49500\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.33371\u001b[0m\u001b[0m | time: 27.869s\n",
      "| SGD | epoch: 017 | loss: 0.33371 - acc: 0.9339 -- iter: 48000/49500\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.30977\u001b[0m\u001b[0m | time: 28.527s\n",
      "| SGD | epoch: 017 | loss: 0.30977 - acc: 0.9387 -- iter: 49000/49500\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.28847\u001b[0m\u001b[0m | time: 30.345s\n",
      "| SGD | epoch: 017 | loss: 0.28847 - acc: 0.9424 | val_loss: 0.11026 - val_acc: 0.9718 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.26993\u001b[0m\u001b[0m | time: 0.593s\n",
      "| SGD | epoch: 018 | loss: 0.26993 - acc: 0.9456 -- iter: 01000/49500\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.25336\u001b[0m\u001b[0m | time: 1.168s\n",
      "| SGD | epoch: 018 | loss: 0.25336 - acc: 0.9484 -- iter: 02000/49500\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.23616\u001b[0m\u001b[0m | time: 1.757s\n",
      "| SGD | epoch: 018 | loss: 0.23616 - acc: 0.9512 -- iter: 03000/49500\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.22206\u001b[0m\u001b[0m | time: 2.327s\n",
      "| SGD | epoch: 018 | loss: 0.22206 - acc: 0.9535 -- iter: 04000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.20951\u001b[0m\u001b[0m | time: 2.901s\n",
      "| SGD | epoch: 018 | loss: 0.20951 - acc: 0.9559 -- iter: 05000/49500\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.19646\u001b[0m\u001b[0m | time: 3.469s\n",
      "| SGD | epoch: 018 | loss: 0.19646 - acc: 0.9583 -- iter: 06000/49500\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.18452\u001b[0m\u001b[0m | time: 4.032s\n",
      "| SGD | epoch: 018 | loss: 0.18452 - acc: 0.9602 -- iter: 07000/49500\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.17573\u001b[0m\u001b[0m | time: 4.605s\n",
      "| SGD | epoch: 018 | loss: 0.17573 - acc: 0.9618 -- iter: 08000/49500\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.16706\u001b[0m\u001b[0m | time: 5.169s\n",
      "| SGD | epoch: 018 | loss: 0.16706 - acc: 0.9629 -- iter: 09000/49500\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.15938\u001b[0m\u001b[0m | time: 5.786s\n",
      "| SGD | epoch: 018 | loss: 0.15938 - acc: 0.9644 -- iter: 10000/49500\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.15016\u001b[0m\u001b[0m | time: 6.359s\n",
      "| SGD | epoch: 018 | loss: 0.15016 - acc: 0.9666 -- iter: 11000/49500\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.14503\u001b[0m\u001b[0m | time: 6.927s\n",
      "| SGD | epoch: 018 | loss: 0.14503 - acc: 0.9672 -- iter: 12000/49500\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.13811\u001b[0m\u001b[0m | time: 7.498s\n",
      "| SGD | epoch: 018 | loss: 0.13811 - acc: 0.9692 -- iter: 13000/49500\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.13444\u001b[0m\u001b[0m | time: 8.074s\n",
      "| SGD | epoch: 018 | loss: 0.13444 - acc: 0.9697 -- iter: 14000/49500\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.12828\u001b[0m\u001b[0m | time: 8.648s\n",
      "| SGD | epoch: 018 | loss: 0.12828 - acc: 0.9712 -- iter: 15000/49500\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.12440\u001b[0m\u001b[0m | time: 8.962s\n",
      "| SGD | epoch: 018 | loss: 0.12440 - acc: 0.9722 -- iter: 16000/49500\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.12250\u001b[0m\u001b[0m | time: 9.269s\n",
      "| SGD | epoch: 018 | loss: 0.12250 - acc: 0.9722 -- iter: 17000/49500\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.11827\u001b[0m\u001b[0m | time: 9.858s\n",
      "| SGD | epoch: 018 | loss: 0.11827 - acc: 0.9733 -- iter: 18000/49500\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.11566\u001b[0m\u001b[0m | time: 10.467s\n",
      "| SGD | epoch: 018 | loss: 0.11566 - acc: 0.9734 -- iter: 19000/49500\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.11210\u001b[0m\u001b[0m | time: 11.042s\n",
      "| SGD | epoch: 018 | loss: 0.11210 - acc: 0.9741 -- iter: 20000/49500\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.10671\u001b[0m\u001b[0m | time: 11.609s\n",
      "| SGD | epoch: 018 | loss: 0.10671 - acc: 0.9755 -- iter: 21000/49500\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.10448\u001b[0m\u001b[0m | time: 12.181s\n",
      "| SGD | epoch: 018 | loss: 0.10448 - acc: 0.9757 -- iter: 22000/49500\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.10103\u001b[0m\u001b[0m | time: 12.833s\n",
      "| SGD | epoch: 018 | loss: 0.10103 - acc: 0.9764 -- iter: 23000/49500\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.10000\u001b[0m\u001b[0m | time: 13.520s\n",
      "| SGD | epoch: 018 | loss: 0.10000 - acc: 0.9764 -- iter: 24000/49500\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.10011\u001b[0m\u001b[0m | time: 14.177s\n",
      "| SGD | epoch: 018 | loss: 0.10011 - acc: 0.9754 -- iter: 25000/49500\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.09984\u001b[0m\u001b[0m | time: 14.828s\n",
      "| SGD | epoch: 018 | loss: 0.09984 - acc: 0.9754 -- iter: 26000/49500\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.09711\u001b[0m\u001b[0m | time: 15.462s\n",
      "| SGD | epoch: 018 | loss: 0.09711 - acc: 0.9760 -- iter: 27000/49500\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.09484\u001b[0m\u001b[0m | time: 16.034s\n",
      "| SGD | epoch: 018 | loss: 0.09484 - acc: 0.9764 -- iter: 28000/49500\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.09533\u001b[0m\u001b[0m | time: 16.602s\n",
      "| SGD | epoch: 018 | loss: 0.09533 - acc: 0.9760 -- iter: 29000/49500\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.09383\u001b[0m\u001b[0m | time: 17.166s\n",
      "| SGD | epoch: 018 | loss: 0.09383 - acc: 0.9765 -- iter: 30000/49500\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.09444\u001b[0m\u001b[0m | time: 17.739s\n",
      "| SGD | epoch: 018 | loss: 0.09444 - acc: 0.9761 -- iter: 31000/49500\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.09150\u001b[0m\u001b[0m | time: 18.303s\n",
      "| SGD | epoch: 018 | loss: 0.09150 - acc: 0.9768 -- iter: 32000/49500\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.09256\u001b[0m\u001b[0m | time: 18.877s\n",
      "| SGD | epoch: 018 | loss: 0.09256 - acc: 0.9758 -- iter: 33000/49500\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.09080\u001b[0m\u001b[0m | time: 19.453s\n",
      "| SGD | epoch: 018 | loss: 0.09080 - acc: 0.9761 -- iter: 34000/49500\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.08944\u001b[0m\u001b[0m | time: 20.022s\n",
      "| SGD | epoch: 018 | loss: 0.08944 - acc: 0.9759 -- iter: 35000/49500\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.08880\u001b[0m\u001b[0m | time: 20.644s\n",
      "| SGD | epoch: 018 | loss: 0.08880 - acc: 0.9760 -- iter: 36000/49500\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.08893\u001b[0m\u001b[0m | time: 21.207s\n",
      "| SGD | epoch: 018 | loss: 0.08893 - acc: 0.9760 -- iter: 37000/49500\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.08918\u001b[0m\u001b[0m | time: 21.789s\n",
      "| SGD | epoch: 018 | loss: 0.08918 - acc: 0.9757 -- iter: 38000/49500\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.08882\u001b[0m\u001b[0m | time: 22.360s\n",
      "| SGD | epoch: 018 | loss: 0.08882 - acc: 0.9759 -- iter: 39000/49500\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.08837\u001b[0m\u001b[0m | time: 22.933s\n",
      "| SGD | epoch: 018 | loss: 0.08837 - acc: 0.9761 -- iter: 40000/49500\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.08820\u001b[0m\u001b[0m | time: 23.495s\n",
      "| SGD | epoch: 018 | loss: 0.08820 - acc: 0.9759 -- iter: 41000/49500\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.08525\u001b[0m\u001b[0m | time: 24.067s\n",
      "| SGD | epoch: 018 | loss: 0.08525 - acc: 0.9767 -- iter: 42000/49500\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.08457\u001b[0m\u001b[0m | time: 24.672s\n",
      "| SGD | epoch: 018 | loss: 0.08457 - acc: 0.9767 -- iter: 43000/49500\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.08310\u001b[0m\u001b[0m | time: 25.242s\n",
      "| SGD | epoch: 018 | loss: 0.08310 - acc: 0.9776 -- iter: 44000/49500\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.08033\u001b[0m\u001b[0m | time: 25.813s\n",
      "| SGD | epoch: 018 | loss: 0.08033 - acc: 0.9782 -- iter: 45000/49500\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.08190\u001b[0m\u001b[0m | time: 26.390s\n",
      "| SGD | epoch: 018 | loss: 0.08190 - acc: 0.9781 -- iter: 46000/49500\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.08112\u001b[0m\u001b[0m | time: 26.958s\n",
      "| SGD | epoch: 018 | loss: 0.08112 - acc: 0.9781 -- iter: 47000/49500\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.07906\u001b[0m\u001b[0m | time: 27.528s\n",
      "| SGD | epoch: 018 | loss: 0.07906 - acc: 0.9790 -- iter: 48000/49500\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.07732\u001b[0m\u001b[0m | time: 28.103s\n",
      "| SGD | epoch: 018 | loss: 0.07732 - acc: 0.9793 -- iter: 49000/49500\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.07832\u001b[0m\u001b[0m | time: 30.028s\n",
      "| SGD | epoch: 018 | loss: 0.07832 - acc: 0.9791 | val_loss: 0.10246 - val_acc: 0.9716 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.07828\u001b[0m\u001b[0m | time: 0.622s\n",
      "| SGD | epoch: 019 | loss: 0.07828 - acc: 0.9784 -- iter: 01000/49500\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.07866\u001b[0m\u001b[0m | time: 1.293s\n",
      "| SGD | epoch: 019 | loss: 0.07866 - acc: 0.9781 -- iter: 02000/49500\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.08073\u001b[0m\u001b[0m | time: 1.867s\n",
      "| SGD | epoch: 019 | loss: 0.08073 - acc: 0.9774 -- iter: 03000/49500\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.07936\u001b[0m\u001b[0m | time: 2.450s\n",
      "| SGD | epoch: 019 | loss: 0.07936 - acc: 0.9777 -- iter: 04000/49500\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.07966\u001b[0m\u001b[0m | time: 3.016s\n",
      "| SGD | epoch: 019 | loss: 0.07966 - acc: 0.9778 -- iter: 05000/49500\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.07967\u001b[0m\u001b[0m | time: 3.611s\n",
      "| SGD | epoch: 019 | loss: 0.07967 - acc: 0.9777 -- iter: 06000/49500\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.08071\u001b[0m\u001b[0m | time: 4.178s\n",
      "| SGD | epoch: 019 | loss: 0.08071 - acc: 0.9774 -- iter: 07000/49500\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.07949\u001b[0m\u001b[0m | time: 4.755s\n",
      "| SGD | epoch: 019 | loss: 0.07949 - acc: 0.9777 -- iter: 08000/49500\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.07687\u001b[0m\u001b[0m | time: 5.362s\n",
      "| SGD | epoch: 019 | loss: 0.07687 - acc: 0.9785 -- iter: 09000/49500\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.07733\u001b[0m\u001b[0m | time: 5.948s\n",
      "| SGD | epoch: 019 | loss: 0.07733 - acc: 0.9788 -- iter: 10000/49500\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.07693\u001b[0m\u001b[0m | time: 6.522s\n",
      "| SGD | epoch: 019 | loss: 0.07693 - acc: 0.9791 -- iter: 11000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.07844\u001b[0m\u001b[0m | time: 7.086s\n",
      "| SGD | epoch: 019 | loss: 0.07844 - acc: 0.9785 -- iter: 12000/49500\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.07543\u001b[0m\u001b[0m | time: 7.647s\n",
      "| SGD | epoch: 019 | loss: 0.07543 - acc: 0.9793 -- iter: 13000/49500\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.07490\u001b[0m\u001b[0m | time: 8.212s\n",
      "| SGD | epoch: 019 | loss: 0.07490 - acc: 0.9791 -- iter: 14000/49500\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.07425\u001b[0m\u001b[0m | time: 8.797s\n",
      "| SGD | epoch: 019 | loss: 0.07425 - acc: 0.9799 -- iter: 15000/49500\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.07582\u001b[0m\u001b[0m | time: 9.365s\n",
      "| SGD | epoch: 019 | loss: 0.07582 - acc: 0.9793 -- iter: 16000/49500\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.07726\u001b[0m\u001b[0m | time: 9.686s\n",
      "| SGD | epoch: 019 | loss: 0.07726 - acc: 0.9791 -- iter: 17000/49500\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.07527\u001b[0m\u001b[0m | time: 10.005s\n",
      "| SGD | epoch: 019 | loss: 0.07527 - acc: 0.9798 -- iter: 18000/49500\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.07151\u001b[0m\u001b[0m | time: 10.615s\n",
      "| SGD | epoch: 019 | loss: 0.07151 - acc: 0.9812 -- iter: 19000/49500\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.07226\u001b[0m\u001b[0m | time: 11.182s\n",
      "| SGD | epoch: 019 | loss: 0.07226 - acc: 0.9806 -- iter: 20000/49500\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.07069\u001b[0m\u001b[0m | time: 11.748s\n",
      "| SGD | epoch: 019 | loss: 0.07069 - acc: 0.9810 -- iter: 21000/49500\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.07060\u001b[0m\u001b[0m | time: 12.343s\n",
      "| SGD | epoch: 019 | loss: 0.07060 - acc: 0.9814 -- iter: 22000/49500\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.07156\u001b[0m\u001b[0m | time: 12.926s\n",
      "| SGD | epoch: 019 | loss: 0.07156 - acc: 0.9811 -- iter: 23000/49500\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.07231\u001b[0m\u001b[0m | time: 13.488s\n",
      "| SGD | epoch: 019 | loss: 0.07231 - acc: 0.9813 -- iter: 24000/49500\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.07225\u001b[0m\u001b[0m | time: 14.063s\n",
      "| SGD | epoch: 019 | loss: 0.07225 - acc: 0.9812 -- iter: 25000/49500\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.07262\u001b[0m\u001b[0m | time: 14.719s\n",
      "| SGD | epoch: 019 | loss: 0.07262 - acc: 0.9813 -- iter: 26000/49500\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.07377\u001b[0m\u001b[0m | time: 15.364s\n",
      "| SGD | epoch: 019 | loss: 0.07377 - acc: 0.9813 -- iter: 27000/49500\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.07645\u001b[0m\u001b[0m | time: 16.024s\n",
      "| SGD | epoch: 019 | loss: 0.07645 - acc: 0.9806 -- iter: 28000/49500\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.07427\u001b[0m\u001b[0m | time: 16.678s\n",
      "| SGD | epoch: 019 | loss: 0.07427 - acc: 0.9809 -- iter: 29000/49500\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.07275\u001b[0m\u001b[0m | time: 17.303s\n",
      "| SGD | epoch: 019 | loss: 0.07275 - acc: 0.9809 -- iter: 30000/49500\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.07291\u001b[0m\u001b[0m | time: 17.881s\n",
      "| SGD | epoch: 019 | loss: 0.07291 - acc: 0.9808 -- iter: 31000/49500\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.07108\u001b[0m\u001b[0m | time: 18.453s\n",
      "| SGD | epoch: 019 | loss: 0.07108 - acc: 0.9812 -- iter: 32000/49500\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.07133\u001b[0m\u001b[0m | time: 19.015s\n",
      "| SGD | epoch: 019 | loss: 0.07133 - acc: 0.9808 -- iter: 33000/49500\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.07187\u001b[0m\u001b[0m | time: 19.593s\n",
      "| SGD | epoch: 019 | loss: 0.07187 - acc: 0.9803 -- iter: 34000/49500\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.07541\u001b[0m\u001b[0m | time: 20.189s\n",
      "| SGD | epoch: 019 | loss: 0.07541 - acc: 0.9790 -- iter: 35000/49500\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.07488\u001b[0m\u001b[0m | time: 20.863s\n",
      "| SGD | epoch: 019 | loss: 0.07488 - acc: 0.9788 -- iter: 36000/49500\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.78882\u001b[0m\u001b[0m | time: 21.517s\n",
      "| SGD | epoch: 019 | loss: 0.78882 - acc: 0.8903 -- iter: 37000/49500\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.78917\u001b[0m\u001b[0m | time: 22.099s\n",
      "| SGD | epoch: 019 | loss: 0.78917 - acc: 0.8731 -- iter: 38000/49500\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.78177\u001b[0m\u001b[0m | time: 22.680s\n",
      "| SGD | epoch: 019 | loss: 0.78177 - acc: 0.8631 -- iter: 39000/49500\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.72954\u001b[0m\u001b[0m | time: 23.253s\n",
      "| SGD | epoch: 019 | loss: 0.72954 - acc: 0.8689 -- iter: 40000/49500\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.68109\u001b[0m\u001b[0m | time: 23.819s\n",
      "| SGD | epoch: 019 | loss: 0.68109 - acc: 0.8749 -- iter: 41000/49500\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.62724\u001b[0m\u001b[0m | time: 24.385s\n",
      "| SGD | epoch: 019 | loss: 0.62724 - acc: 0.8834 -- iter: 42000/49500\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.57726\u001b[0m\u001b[0m | time: 24.948s\n",
      "| SGD | epoch: 019 | loss: 0.57726 - acc: 0.8921 -- iter: 43000/49500\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.53203\u001b[0m\u001b[0m | time: 25.531s\n",
      "| SGD | epoch: 019 | loss: 0.53203 - acc: 0.8998 -- iter: 44000/49500\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.49056\u001b[0m\u001b[0m | time: 26.088s\n",
      "| SGD | epoch: 019 | loss: 0.49056 - acc: 0.9065 -- iter: 45000/49500\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.45220\u001b[0m\u001b[0m | time: 26.661s\n",
      "| SGD | epoch: 019 | loss: 0.45220 - acc: 0.9133 -- iter: 46000/49500\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.41501\u001b[0m\u001b[0m | time: 27.219s\n",
      "| SGD | epoch: 019 | loss: 0.41501 - acc: 0.9203 -- iter: 47000/49500\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.38555\u001b[0m\u001b[0m | time: 27.785s\n",
      "| SGD | epoch: 019 | loss: 0.38555 - acc: 0.9248 -- iter: 48000/49500\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.35753\u001b[0m\u001b[0m | time: 28.357s\n",
      "| SGD | epoch: 019 | loss: 0.35753 - acc: 0.9298 -- iter: 49000/49500\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.33114\u001b[0m\u001b[0m | time: 29.983s\n",
      "| SGD | epoch: 019 | loss: 0.33114 - acc: 0.9346 | val_loss: 0.10532 - val_acc: 0.9747 -- iter: 49500/49500\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.30768\u001b[0m\u001b[0m | time: 0.647s\n",
      "| SGD | epoch: 020 | loss: 0.30768 - acc: 0.9388 -- iter: 01000/49500\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.28581\u001b[0m\u001b[0m | time: 1.302s\n",
      "| SGD | epoch: 020 | loss: 0.28581 - acc: 0.9430 -- iter: 02000/49500\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.26468\u001b[0m\u001b[0m | time: 1.968s\n",
      "| SGD | epoch: 020 | loss: 0.26468 - acc: 0.9472 -- iter: 03000/49500\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.24640\u001b[0m\u001b[0m | time: 2.643s\n",
      "| SGD | epoch: 020 | loss: 0.24640 - acc: 0.9506 -- iter: 04000/49500\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.23196\u001b[0m\u001b[0m | time: 3.273s\n",
      "| SGD | epoch: 020 | loss: 0.23196 - acc: 0.9528 -- iter: 05000/49500\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.21677\u001b[0m\u001b[0m | time: 3.850s\n",
      "| SGD | epoch: 020 | loss: 0.21677 - acc: 0.9553 -- iter: 06000/49500\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.20427\u001b[0m\u001b[0m | time: 4.427s\n",
      "| SGD | epoch: 020 | loss: 0.20427 - acc: 0.9574 -- iter: 07000/49500\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.19156\u001b[0m\u001b[0m | time: 4.991s\n",
      "| SGD | epoch: 020 | loss: 0.19156 - acc: 0.9602 -- iter: 08000/49500\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.17986\u001b[0m\u001b[0m | time: 5.618s\n",
      "| SGD | epoch: 020 | loss: 0.17986 - acc: 0.9622 -- iter: 09000/49500\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.17188\u001b[0m\u001b[0m | time: 6.187s\n",
      "| SGD | epoch: 020 | loss: 0.17188 - acc: 0.9637 -- iter: 10000/49500\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.16071\u001b[0m\u001b[0m | time: 6.759s\n",
      "| SGD | epoch: 020 | loss: 0.16071 - acc: 0.9661 -- iter: 11000/49500\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.15249\u001b[0m\u001b[0m | time: 7.332s\n",
      "| SGD | epoch: 020 | loss: 0.15249 - acc: 0.9677 -- iter: 12000/49500\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.14476\u001b[0m\u001b[0m | time: 7.896s\n",
      "| SGD | epoch: 020 | loss: 0.14476 - acc: 0.9690 -- iter: 13000/49500\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.13851\u001b[0m\u001b[0m | time: 8.471s\n",
      "| SGD | epoch: 020 | loss: 0.13851 - acc: 0.9696 -- iter: 14000/49500\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.13340\u001b[0m\u001b[0m | time: 9.035s\n",
      "| SGD | epoch: 020 | loss: 0.13340 - acc: 0.9703 -- iter: 15000/49500\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.12777\u001b[0m\u001b[0m | time: 9.620s\n",
      "| SGD | epoch: 020 | loss: 0.12777 - acc: 0.9712 -- iter: 16000/49500\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.12386\u001b[0m\u001b[0m | time: 10.186s\n",
      "| SGD | epoch: 020 | loss: 0.12386 - acc: 0.9719 -- iter: 17000/49500\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.11895\u001b[0m\u001b[0m | time: 10.506s\n",
      "| SGD | epoch: 020 | loss: 0.11895 - acc: 0.9727 -- iter: 18000/49500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.11406\u001b[0m\u001b[0m | time: 10.825s\n",
      "| SGD | epoch: 020 | loss: 0.11406 - acc: 0.9733 -- iter: 19000/49500\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.10725\u001b[0m\u001b[0m | time: 11.397s\n",
      "| SGD | epoch: 020 | loss: 0.10725 - acc: 0.9749 -- iter: 20000/49500\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.10426\u001b[0m\u001b[0m | time: 11.962s\n",
      "| SGD | epoch: 020 | loss: 0.10426 - acc: 0.9754 -- iter: 21000/49500\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.10268\u001b[0m\u001b[0m | time: 12.536s\n",
      "| SGD | epoch: 020 | loss: 0.10268 - acc: 0.9757 -- iter: 22000/49500\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.09888\u001b[0m\u001b[0m | time: 13.099s\n",
      "| SGD | epoch: 020 | loss: 0.09888 - acc: 0.9766 -- iter: 23000/49500\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.09684\u001b[0m\u001b[0m | time: 13.669s\n",
      "| SGD | epoch: 020 | loss: 0.09684 - acc: 0.9766 -- iter: 24000/49500\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.09505\u001b[0m\u001b[0m | time: 14.245s\n",
      "| SGD | epoch: 020 | loss: 0.09505 - acc: 0.9767 -- iter: 25000/49500\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.09502\u001b[0m\u001b[0m | time: 14.811s\n",
      "| SGD | epoch: 020 | loss: 0.09502 - acc: 0.9766 -- iter: 26000/49500\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.09265\u001b[0m\u001b[0m | time: 15.408s\n",
      "| SGD | epoch: 020 | loss: 0.09265 - acc: 0.9770 -- iter: 27000/49500\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.09182\u001b[0m\u001b[0m | time: 15.977s\n",
      "| SGD | epoch: 020 | loss: 0.09182 - acc: 0.9769 -- iter: 28000/49500\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.09013\u001b[0m\u001b[0m | time: 16.627s\n",
      "| SGD | epoch: 020 | loss: 0.09013 - acc: 0.9772 -- iter: 29000/49500\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.08954\u001b[0m\u001b[0m | time: 17.298s\n",
      "| SGD | epoch: 020 | loss: 0.08954 - acc: 0.9775 -- iter: 30000/49500\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.08830\u001b[0m\u001b[0m | time: 17.972s\n",
      "| SGD | epoch: 020 | loss: 0.08830 - acc: 0.9779 -- iter: 31000/49500\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.08766\u001b[0m\u001b[0m | time: 18.639s\n",
      "| SGD | epoch: 020 | loss: 0.08766 - acc: 0.9778 -- iter: 32000/49500\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.08702\u001b[0m\u001b[0m | time: 19.264s\n",
      "| SGD | epoch: 020 | loss: 0.08702 - acc: 0.9773 -- iter: 33000/49500\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.08559\u001b[0m\u001b[0m | time: 19.833s\n",
      "| SGD | epoch: 020 | loss: 0.08559 - acc: 0.9775 -- iter: 34000/49500\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.08860\u001b[0m\u001b[0m | time: 20.455s\n",
      "| SGD | epoch: 020 | loss: 0.08860 - acc: 0.9768 -- iter: 35000/49500\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.08777\u001b[0m\u001b[0m | time: 21.016s\n",
      "| SGD | epoch: 020 | loss: 0.08777 - acc: 0.9768 -- iter: 36000/49500\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.08844\u001b[0m\u001b[0m | time: 21.592s\n",
      "| SGD | epoch: 020 | loss: 0.08844 - acc: 0.9765 -- iter: 37000/49500\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.08813\u001b[0m\u001b[0m | time: 22.168s\n",
      "| SGD | epoch: 020 | loss: 0.08813 - acc: 0.9768 -- iter: 38000/49500\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.08759\u001b[0m\u001b[0m | time: 22.734s\n",
      "| SGD | epoch: 020 | loss: 0.08759 - acc: 0.9767 -- iter: 39000/49500\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.08604\u001b[0m\u001b[0m | time: 23.310s\n",
      "| SGD | epoch: 020 | loss: 0.08604 - acc: 0.9773 -- iter: 40000/49500\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.08467\u001b[0m\u001b[0m | time: 23.877s\n",
      "| SGD | epoch: 020 | loss: 0.08467 - acc: 0.9778 -- iter: 41000/49500\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.08387\u001b[0m\u001b[0m | time: 24.447s\n",
      "| SGD | epoch: 020 | loss: 0.08387 - acc: 0.9777 -- iter: 42000/49500\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.08266\u001b[0m\u001b[0m | time: 25.013s\n",
      "| SGD | epoch: 020 | loss: 0.08266 - acc: 0.9779 -- iter: 43000/49500\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.08208\u001b[0m\u001b[0m | time: 25.617s\n",
      "| SGD | epoch: 020 | loss: 0.08208 - acc: 0.9786 -- iter: 44000/49500\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.08122\u001b[0m\u001b[0m | time: 26.184s\n",
      "| SGD | epoch: 020 | loss: 0.08122 - acc: 0.9786 -- iter: 45000/49500\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.08045\u001b[0m\u001b[0m | time: 26.768s\n",
      "| SGD | epoch: 020 | loss: 0.08045 - acc: 0.9786 -- iter: 46000/49500\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.07931\u001b[0m\u001b[0m | time: 27.342s\n",
      "| SGD | epoch: 020 | loss: 0.07931 - acc: 0.9787 -- iter: 47000/49500\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.07775\u001b[0m\u001b[0m | time: 27.902s\n",
      "| SGD | epoch: 020 | loss: 0.07775 - acc: 0.9792 -- iter: 48000/49500\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.07753\u001b[0m\u001b[0m | time: 28.477s\n",
      "| SGD | epoch: 020 | loss: 0.07753 - acc: 0.9790 -- iter: 49000/49500\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.07796\u001b[0m\u001b[0m | time: 30.123s\n",
      "| SGD | epoch: 020 | loss: 0.07796 - acc: 0.9789 | val_loss: 0.08655 - val_acc: 0.9767 -- iter: 49500/49500\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_inputs=trainX,\n",
    "          Y_targets=trainY,\n",
    "          n_epoch=20,\n",
    "          batch_size=1000,\n",
    "          validation_set=0.1,\n",
    "          show_metric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習させたモデルを用いてテスト用画像ピクセルデータの予測を行う。\n",
    "\n",
    "それぞれの出力結果は0〜9とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.array(model.predict(testX)).argmax(axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト用データ正解を値をlabelに格納する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = testY.argmax(axis=1)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97709999999999997"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.mean(pred == label, axis=0)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルの保存と読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みのモデルの保存や読み込みを行うことができる。  \n",
    "読み込んだモデルは同様にテストデータに対して予測を行うことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/Users/fukunaritakeshi/PycharmProjects/python-ml-programming/handwritten_rnn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"handwritten_rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/fukunaritakeshi/PycharmProjects/python-ml-programming/handwritten_rnn\n"
     ]
    }
   ],
   "source": [
    "new_model = model.load(\"handwritten_rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9771\n"
     ]
    }
   ],
   "source": [
    "new_pred = np.array(model.predict(testX)).argmax(axis=1)\n",
    "label = testY.argmax(axis=1)\n",
    "new_accuracy = np.mean(new_pred == label, axis=0)\n",
    "new_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
